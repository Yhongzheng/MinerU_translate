2024-08-19 16:18:07,044 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1.md: 文件路径无效: # Many-Shot In-Context Learning in Multimodal Foundation Models  

# Yixing Jiang \u2217 Jeremy Irvin \u2217 Ji Hun Wang Muhammad Ahmed Chaudhry Jonathan H. Chen Andrew Y. Ng  

Stanford University {jiang6,jirvin16,jihunwang,ang}@cs.stanford.edu {mahmedch,jonc101}@stanford.edu  

# Abstract  

Large language models are well-known to be effective at few-shot in-context learning (ICL). Recent advancements in multimodal foundation models have en- abled unprecedentedly long context windows, presenting an opportunity to explore their capability to perform ICL with many more demonstrating examples. In this work, we evaluate the performance of multimodal foundation models scaling from few-shot to many-shot ICL. We benchmark GPT-4o and Gemini   $1.5\ \mathrm{Pr}\bar{\mathrm{o}}$  across 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification). We observe that many-shot ICL, including up to al- most 2,000 multimodal demonstrating examples, leads to substantial improvements compared to few-shot   $\langle<\!100$   examples) ICL across all of the datasets. Further, Gemini   $1.5~\mathrm{Pro}$   performance continues to improve log-linearly up to the max- imum number of tested examples on many datasets. Given the high inference costs associated with the long prompts required for many-shot ICL, we also ex- plore the impact of batching multiple queries in a single API call. We show that batching up to 50 queries can lead to performance improvements under zero-shot and manyCshot ICL, with substantial gains in the zero-shot setting on multiple datasets, while drastically reducing per-query cost and latency. Finally, we measure ICL data efficiency of the models, or the rate at which the models learn from more demonstrating examples. We find that while GPT-4o and Gemini   $1.5~\mathrm{Pro}$  achieve similar zero-shot performance across the datasets, Gemini 1.5 Pro exhibits higher ICL data efficiency than GPT-4o on most datasets. Our results suggest that many-shot ICL could enable users to efficiently adapt multimodal foundation models to new applications and domains. Our codebase is publicly available at https://github.com/stanfordmlgroup/ManyICL .  

# 1 Introduction  

Large language models (LLMs) have been shown to substantially benefit from the inclusion of a few demonstrating examples ( shots ) in the LLM context before the test query [ 1 C 3 ]. This phenomenon, commonly referred to as in-context learning (ICL), enables LLMs to learn from few shots without any updates to model parameters, and therefore improves specialization to new tasks without any further model training. More recently, large multimodal models (LMMs) have also demonstrated the capability of learning from in-context examples [4C6]. Han et al. [5] and Zhang et al. [6] both show that few-shot multimodal ICL specifically helps to improve LMM performance on out-domain or out-of-distribution tasks.  

![](images/c2295c09026975f63765663f5663420aa41ecabe03fed3062728054bfde7fa4a.jpg)  
Figure 1:  Many-shot multimodal in-context learning compared to zero-shot and few-shot multimodal ICL.  In zero-shot and few-shot settings, respectively, no demonstrating examples or only a small number of demonstrating examples are provided in the context before the test query. In a many-shot ICL setting, we include a large number of demonstrating examples in the prompt, whereas in batched many-shot ICL, we perform multiple queries at once using query references.  

While few-shot ICL has enabled promising performance improvements for both LLMs and LMMs, limited model context windows have constrained research on the impact of increasing the number of demonstrating examples on performance. This is especially true for LMMs as most use a large number of visual tokens to represent images. However, due to recent advancements enabling substantially longer context windows C for example, 128,000 tokens for GPT-4o and up to one million tokens for Gemini 1.5 Pro C it is now possible to explore the effect of drastically increasing the number of demonstrating examples.  

To investigate the capability of state-of-the-art multimodal foundation models to perform many-shot ICL, we conduct a large suite of experiments benchmarking model performance on 10 datasets spanning several domains and image classification tasks after scaling up the number of demonstrating examples by multiple orders of magnitude. Specifically, our contributions are as follows:  

1.  We show that providing multimodal foundation models with many demonstrating examples leads to substantial performance improvements compared to providing only a few demon- strating examples. We observe that the performance of Gemini 1.5 Pro generally improves log-linearly as the number of demonstrating examples increases, whereas GPT-4o exhibits less stable improvements as the number of in-context examples increases. 2.  We measure the data efficiency of the models under ICL as the number of demonstrating examples is increased, and find that Gemini  $1.5\;\mathrm{Pro}$   exhibits higher ICL data efficiency than GPT-4o on most datasets. 3.  We demonstrate that batching multiple queries into a single request can achieve similar or better performance than single query requests in a many-shot setting, while enabling substantially lower per-example latency and much cheaper per-example inference cost. 4.  We find that batching multiple questions can lead to substantial performance improvements in a zero-shot setting. We design experiments to explain this phenomenon, and find that the improvements are due to a combination of domain calibration, class calibration, and self-generated demonstrating examples due to autoregressive decoding.  

![](images/37c467c4191e2fac2d692a0fab388edb7e621823aa9ac0d60e897b50ce2cfee8.jpg)  

# 2 Related Work  

Scaling ICL.  The seminal work of Brown et al.  [1]  discovered performance improvements for LLMs from increasing the number of in-context examples, but the tested number of demonstrating examples was low (10 to 100), likely due to the restrictive context size (2048 tokens for GPT3). Increasing the number of in-context examples has only been explored recently by a few works [ 7 C 9 ]. Both Li et al.  [7]  and Agarwal et al.  [8]  explore scaling in-context learning to more than 1,000 demonstrating examples and find performance improvements across multiple tasks. However, their experiments are limited to text-only benchmarks and do not compare performance across different models.  

Multimodal ICL.  Due to the recent emergence of LMMs, research on multimodal ICL is still nascent. One prior work developed a new model to leverage complex prompts composed of multimodal inputs in order to allow models to compare images [ 10 ], while other recent works explored the general iz ability of GPT-4V and Gemini to multimodal out-domain and out-of-distribution tasks, and found that ICL leads to performance benefits for both models across many tasks [ 6 ,  5 ]. However, none of these works have leveraged the new largely expanded context windows to investigate the effects of increasing the number of demonstrating examples.  

Batch Querying.  Multiple prior works have explored batching queries (also commonly referred to as batch prompting) for more efficient and cheaper inference. Batch prompting was first introduced in Cheng et al.  [11] , leading to comparable or better performance than single prompting, while achieving substantially reduced inference token cost and latency. Lin et al.  [12]  observe performance degradation with batched prompts in longer contexts, and propose a variety of techniques to mitigate the performance loss. More recently, additional variations of batch prompting have been proposed, including grouping similar questions together [ 13 ], batching prompts of different tasks [ 14 ], and concatenating multiple images into a single image collage [ 15 ]. We again note that batch prompting with high numbers of demonstrating examples and high numbers of queries has only become feasible due to larger context windows of recent models.  

# 3 Methods  

We conduct several experiments to test the effect of increasing the number of demonstrating examples on the performance of two state-of-the-art multimodal foundation models: GPT-4o and Gemini 1.5 Pro (Section 3.1). We benchmark their performance using standard performance metrics as well as an ICL data efficiency metric (Section 3.3) on 10 datasets spanning several vision domains and image classification tasks (Section 3.2). We conduct ablation studies to test the impact of batching queries on model performance and explain the substantial improvement in zero-shot settings (Section 4.2). We refer to the many-shot in-context learning framework as many-shot ICL. Figure 1 provides an illustrative summary of many-shot ICL and batched many-shot ICL compared to zero-shot and few-shot ICL.  

# 3.1 Models  

We use three state-of-the-art multimodal foundation models with public API access, namely GPT-4o, GPT4(V)-Turbo [ 4 ], and Gemini 1.5 Pro [ 16 ]. Because GPT-4o performs substantially better than GPT4(V)-Turbo, we focus on the results of GPT-4o and Gemini 1.5 Pro in the main text, and include GPT4(V)-Turbo results in the Appendix. We do not utilize Claude3-Opus in our experiments, as it only accepts up to 20 images in one request at the time of writing. The specific endpoint for for GPT-4o is “gpt-4o-2024-05-13”, for GPT-4(V)-Turbo is “gpt-4-turbo-2024-04-09”, and for Gemini  $1.5\;\mathrm{Pro}$   is “gemini-1.5-pro-preview-  $.0409^{\ast}$  . We use the API service provided by OpenAI for GPT-4o and GPT-4(V)-Turbo, and the API service provided by Google Cloud on Vertex AI for Gemini 1.5 Pro. We set the temperature to zero for all models and a random seed for GPT-4(V)-Turbo and GPT-4o to obtain more deterministic responses. To prevent models from abstaining (which happens rarely), we rerun the query until an answer is provided.  

# 3.2 Datasets  

We benchmark the model performance on 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification). We choose to focus on image classification tasks as other tasks such as region captioning would require substantially more tokens thereby limiting the total number of demonstrating examples, and most LMMs are not yet capable of accurately producing localizations required for other tasks like bounding boxes and segmentation masks [ 17 ,  18 ]. Table 1 provides a summary of the datasets used in this study.  

For all datasets, we construct a set of demonstration (demo) examples from the original training and validation splits used for in-context learning and a test set from the original test split (if one exists) to evaluate the performance of the models. We randomly sample the demo and test sets from the original dataset without replacement. For the multi-class and fine-grained classification datasets, we perform a class-stratified sampling, ensuring an equal number of examples per class in both the demo and test sets. For the multi-label classification dataset (CheXpert), we sample an equal number of positive and negative samples per class in both the demo and test sets. We note that, since the task is multi-label, this sampling procedure does not result in an exactly equal number of examples per class. The per-dataset sizes of the full demo and test sets are shown in Table 1, and we increase the number of demonstration examples up to the numbers shown in the table while ensuring class balance for the scaling experiments.  

# 3.3 Evaluation Metrics  

We use standard metrics to evaluate model performance on each dataset. Specifically, we measure performance using accuracy for all multi-class classification datasets as they are sampled to have a balanced class distribution. For multi-label classification on CheXpert, we use the macro-averaged F1 metric. In the rare case of parsing errors, we consider the response as incorrect. To estimate the variability around the evaluation metrics, we compute standard deviation using bootstrapping with 1,000 bootstrap replicates.  

In addition to standard performance metrics, we measure the data efficiency of each model. Specifi- cally, we compute a linear regression between  $\log_{10}(N+1)$   (with    $N$   the number of examples) and model performance, enforcing that the line passes through the zero-shot performance point. This value approximates the amount of performance improvement from zero-shot expected from including an order of magnitude more demonstrating examples.  

Table 1:  Summary of benchmark datasets.  We use 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification).  

![](images/7205245d4d93aea120e20581767010eac80e677a2b8fa68813b14b64b9b19c77.jpg)  

![Table 2:  Many-shot ICL performance and efficiency comparison.  We report the performance under a zero-shot regime and performance at the optimal demo set size as well as the many-shot ICL data efficiency of GPT-4o and Gemini  $1.5~\mathrm{Pro}$  . We measure performance using accuracy on all datasets except CheXpert, for which we use macro-average F1. We bold the highest ICL data efficiency between the two models on each dataset. ](images/5747989fbf490aa5015d41f600c67f18638cc9a72f5f650cee24797078ec72b4.jpg)  

# 4 Results  

We present many-shot ICL performance using batched queries in Section 4.1, investigate the impact of batching queries on performance in Section 4.2, and provide an analysis on cost and latency in Section 4.3. Results using GPT4(V)-Turbo are in Appendix C.  

# 4.1 Increasing number of demonstrating examples  

Main Results.  Gemini 1.5 Pro exhibits consistent and substantial improvements as the number of demonstrating examples increases across all datasets except for DrugOOD Assay (Figure 2). Gemini  $1.5\;\mathrm{Pro}$   shows particularly large improvements from many-shot ICL on HAM10000 (  ${\it+23\%}$   accuracy compared to zero-shot,  $+16\%$   compared to 7 examples), FIVES   ${\it\Omega}+29\%$   compared to zero-shot,  $+27\%$  compared to 20 examples), and EuroSAT   $(+38\%$   compared to zero-shot,  $+31\%$   compared to 10 examples). Notably, for 5 out of the 10 datasets (FIVES, UCMerced, EuroSAT, Oxford Pets, and DTD), Gemini 1.5 Pro performance continues to improve up to the highest number of demonstrating examples considered   $(\mathord{\sim}1,\!000$   examples). On the other 5 datasets, the optimal performance occurs prior to the highest number of demo examples, with the maximum number of demo examples leading to similar or slightly worse performance than the optimal demo set size. On the other hand, Gemini 1.5 Pro performance on DrugOOD Assay does not substantially benefit from many-shot ICL, with high variance in performance across demo sizes and the peak performance at 40 demo examples.  

Similarly, GPT-4o shows substantial performance improvements on all datasets except FIVES and DrugOOD Assay using many-shot ICL, but the improvement is not consistent. For many datasets, performance drops sharply at first and then improves significantly as the number of demonstrating examples increases further, resulting in V-shaped scaling curves (Figure 2). We also note that we were unable to increase the number of demo examples to the same level as considered for Gemini 1.5 Pro because GPT-4o has a shorter context window and is more prone to timeout errors with longer inputs. GPT-4o performance on DrugOOD Assay shows high variance, similar to Gemini  $1.5\;\mathrm{Pro}$  , with the peak performance observed at 50 demo examples.  

Sensitivity to prompt selection.  We also explore a different set of prompts to test the robustness of many-shot ICL to differences in prompt wording on two datasets. While there is a small deviation in performance between different prompts, the overall log-linear improvement trend is consistent across the prompts. Details can be found in Appendix B.  

ICL data efficiency.  We find Gemini   $1.5\;\mathrm{Pro}$   demonstrates higher ICL data efficiency than GPT- 4o across all datasets except TerraIncognita and DTD (Table 2). Gemini 1.5 Pro ICL efficiency is especially high on EuroSAT, with  $20.61\%$   improvement in accuracy for every   $10\mathbf{x}$   more demo examples, and lowest on DrugOOD Assay (2.03), Camelyon17 (3.00), and TerraIncognita (3.50). GPT-4o ICL data efficiency is especially high on TerraIncognita   $(20.50\%)$   and EuroSat (19.40).  

![](images/bff391610e9c220cf73f2868432fb8d09d38554b2955e4c0cc37c21cee960108.jpg)  
Figure 3:  Gemini 1.5 Pro performance under many-shot and zero-shot ICL when varying the amount of queries included in every request.  We show performance per batch size with the optimal number of demo examples (many-shot) and no demo examples (zero-shot). The  $x$  -axis is in log scale. Under the many-shot regime, batching queries leads to no substantial drop in performance compared to individual queries when we choose a suitable batch size. For zero-shot, including only one query is suboptimal for many datasets.  

Gemini 1.5 Pro has a positive efficiency on all datasets and GPT-4o has a positive data efficiency on 9 of the 10 datasets (excluding Oxford Pets). Importantly, both models benefit substantially from many-shot ICL at the optimal demo set size, with an average improvement of  $+17\%$   for both Gemini 1.5 Pro and GPT-4o.  

# 4.2 Impact of batching queries  

As including a large set of demo examples in the prompt leads to much longer sequence lengths and therefore higher inference time and cost, we consider batching queries in a single prompt to reduce per-query cost, and examine the impact of different batch sizes on model performance. Due to its superior performance and free preview access, we use Gemini   $1.5\;\mathrm{Pro}$   for these experiments.  

Main Results.  We find minimal performance degradations, and sometimes performance improve- ments, as we increase the number of queries included in each batch across under both zero-shot and many-shot (at the optimal demo set size) regimes (Figure 3). Notably, using a single query each time with many-shot ICL is suboptimal across many of the datasets. We find that the optimal batch size is among the three largest sizes on every dataset except CheXpert and EuroSAT, which both see optimal performance with a single query at a time.  

We additionally observe that including a single query at a time is suboptimal on most datasets in the zero-shot regime. Surprisingly, performance with the highest batch size is substantially higher across three datasets under the zero-shot regime, with a consistent performance improvement as the batch size is increased on both UCMerced and Terraincognita.  

Zero-shot performance improvements from batching queries.  We conduct several additional experiments to investigate why batch querying can lead to large performance improvements under the zero-shot regime on TerraIncognita and UCMerced. We hypothesize that this improvement may be due to three potential benefits from ICL: (1) domain calibration, where the model benefits from seeing more images in the domain in order to adapt to it, (2) class calibration, where seeing images of different classes enables the model to better calibrate its outputs, and (3) self-ICL (shown to be effective in prior work [ 29 ]), where the model can learn from self-generated demonstrations due to autoregressive decoding. We design experiments to isolate the potential benefits from each of these types of ICL between asking a single query to batching 50 queries together.  

First, to measure potential improvement from domain calibration, we include 49 images from the same class in the prompt without including any label. We find a  $3.0\%$   improvement on TerraIncognita  

![](images/d448fc079ac55d9535c79b6dcb9cd01f084aad34cad1d391283da4d8103570e9.jpg)  
Figure 4:  Ablation study to investigate why batching queries leads to performance improve- ments when using Gemini  $\mathbf{1.5\,Pr0}$   in a zero-shot setting.  The first bar shows performance when including a single query, the second adds 49 unlabeled images from a single class, the third adds 49 unlabeled images in total from all classes, the fourth adds model responses to include self-generated demonstrations, and the last includes 50 queries in one request.  

and  $2.6\%$   degradation on UCMerced, suggesting domain calibration is helpful for the former but not the latter. Second, to capture performance gains from class calibration, we include a random sample of 49 images in the prompt, again without including the label. We see a further  $3.5\%$   improvement on TerraIncognita (  $6.5\%$   improvement from a single query) and a  $4.5\%$   improvement from a single query on UCMerced, suggesting including the context of class-balanced images is helpful even without labels. Third, to capture additional performance improvements from the self-generated labels, we obtain predicted labels from the zero-shot model using a single query for each of the 49 randomly sampled images and add them to the prompt. We observe further performance increase on both datasets, with  $5.5\%$   on TerraIncognita and  $2.7\%$   on UCMerced. The final total accuracy is similar to asking the 50 questions each round, which suggests these three components mostly explain the reason for improved zero-shot performance under a larger query batch size.  

# 4.3 Cost and latency analysis  

Many-shot ICL incurs zero additional training cost, but per-query inference can be costly and slow due to long input contexts. To quantitatively measure this, we compute the latency and cost associated with the zero-shot and many-shot requests with and without batching when using Gemini   $1.5\;\mathrm{Pro}$   on HAM10000 and TerraIncognita. We calculate the costs using the Gemini 1.5 Pro preview pricing (  $\S7$   per 1 million input tokens and   $\S21$   per 1 million output tokens). For fair comparison and to minimize data transfer artifacts, all requests are sent to the same location where the VM instance is held (“us-central1”). We run the query three times under each setting and report the average.  

In the zero-shot regime, we see substantial per-example latency reductions due to query batching, close to a  $10\mathbf{x}$   reduction on HAM10000 and  $2\mathbf{X}$   on TerraIncognita (Table 3). The per-example cost is similar between the two as there is no additional context needed for including demonstrating examples. In the many-shot regime, we observe substantial reductions in both per-example latency and cost on both datasets. Specifically, for HAM10000, we find a near  $35\mathrm{x}$   reduction in latency and  $10\mathbf{x}$   reduction in cost, and   $20\mathbf{x}$   reduction in latency and  $45\mathbf{x}$   reduction in cost for TerraIncognita.  

# 5 Discussion  

In this study, we evaluate many-shot ICL of state-of-the-art multimodal foundation models across 10 datasets and find consistent performance improvements across most of the datasets. Batching queries with many-shot ICL further exhibits substantially reduced per-example latency and inference costs without compromising performance.  

Our findings suggest that these multimodal foundation models have the capability of performing ICL with large numbers of demonstrating examples, which may have significant implications on their practical use. For example, it was previously impossible to adapt these large, private models to  

![Table 3:  Inference latency and cost using Gemini 1.5 Pro with and without query batching.  We use 50 queries per batch. In the zero-shot setting, we can achieve lower per-example latency with batching, but the per-example cost remains identical. In the many-shot setting, the per-example cost and per-example latency both drop substantially with query batching. ](images/08cad9838361c7c5255cbdae6c004feabab7f6d3d3cb985b2b076061aef4f1a1.jpg)  

new tasks and domains, but many-shot ICL would enable users to leverage demonstrating examples to adapt the models. One significant advantage of many-shot ICL is its ability to get quick results even on the same day of model release, and that’s why we can finish our evaluation using GPT-4o within days. Furthermore, fine-tuning open-source models is the standard practice when practitioners have access to moderately sized datasets, but many-shot ICL may remove the need for fine-tuning, making it much easier to develop customized approaches. We note that it remains to be seen how traditional fine-tuning of these models compares to many-shot ICL with foundation models in terms of absolute performance and data efficiency, so future work should explore this. In addition, it is important to study general issues which plague those foundation models, such as hallucinations and biases, under the context of many-shot ICL and batching queries. For example, it would be interesting to explore if carefully curated and large sets of demonstrating examples can reduce biases across different sub-groups. We leave this to future work.  

Our study has limitations. First, we only explore performance under many-shot ICL on image classification tasks and with private foundation models. We believe these are the most practically relevant and common multimodal settings, but it is worthwhile for future work to explore potential benefits from many-shot ICL on other tasks and with upcoming open-source multimodal foundation models like LLaMA-3 [ 30 ]. Second, even after recent developments to increase context size, the size prohibits many-shot ICL from being used on datasets with a large number (several hundred or more) of classes. We anticipate that context window sizes will continue to increase in size over time which will mitigate this issue. Third, the datasets which were used to train these private models have not been disclosed, so it is difficult to tell whether the models have been trained on the datasets we selected. We argue that zero-shot performance across the datasets is far from perfect which provides evidence that the datasets have not been used for training, but we cannot determine that with certainty.  

# 6 Conclusion  

In summary, we show that multimodal foundation models are capable of many-shot ICL. We believe that these results pave a promising path forward to improve the adaptability and accessibility of large multimodal foundation models.  

# References  

[1]  Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.  

[2]  Archit Parnami and Minwoo Lee. Learning from few examples: A summary of approaches to few-shot learning.  arXiv preprint arXiv:2203.04291 , 2022.  

[3]  Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples: A survey on few-shot learning.  ACM computing surveys (csur) , 53(3):1C34, 2020.

 [4]  Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.  arXiv preprint arXiv:2303.08774 , 2023.

 [5]  Zhongyi Han, Guanglin Zhou, Rundong He, Jindong Wang, Xing Xie, Tailin Wu, Yilong Yin, Salman Khan, Lina Yao, Tongliang Liu, et al. How well does gpt-4v (ision) adapt to distribution shifts? a preliminary investigation.  arXiv preprint arXiv:2312.07424 , 2023.

 [6]  Xingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia Hai, Renzhe Xu, Yuqing Yang, Shikai Guan, Jiazheng Xu, and Peng Cui. On the out-of-distribution generalization of multimodal large language models.  arXiv preprint arXiv:2402.06599 , 2024.

 [7]  Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, Zhiyong Wu, and Lingpeng Kong. In-context learning with many demonstration examples.  arXiv preprint arXiv:2302.04931 , 2023.

 [8]  Rishabh Agarwal, Avi Singh, Lei M Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John D Co-Reyes, Eric Chu, et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018 , 2024.

 [9]  Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R Gormley, and Graham Neubig. In-context learning with long-context models: An in-depth exploration.  arXiv preprint arXiv:2405.00200 , 2024.

 [10]  Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with multi-modal in-context learning.  arXiv preprint arXiv:2309.07915 , 2023.

 [11]  Zhoujun Cheng, Jungo Kasai, and Tao Yu. Batch prompting: Efficient inference with large language model apis.  arXiv preprint arXiv:2301.08721 , 2023.

 [12]  Jianzhe Lin, Maurice Diesendruck, Liang Du, and Robin Abraham. Batchprompt: Accomplish more with less.  arXiv preprint arXiv:2309.00384 , 2023.

 [13]  Jiayi Liu, Tinghan Yang, and Jennifer Neville. Cliqueparcel: An approach for batching llm prompts that jointly optimizes efficiency and faithfulness.  arXiv preprint arXiv:2402.14833 , 2024.

 [14]  Guijin Son, Sangwon Baek, Sangdae Nam, Ilgyun Jeong, and Seungone Kim. Multi-task inference: Can large language models follow multiple instructions at once?  arXiv preprint arXiv:2402.11597 , 2024.

 [15]  Siyu Xu, Yunke Wang, Daochang Liu, and Chang Xu. Collage prompting: Budget-friendly visual recognition with gpt-4v.  arXiv preprint arXiv:2403.11468 , 2024.

 [16]  Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean- baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.  arXiv preprint arXiv:2403.05530 , 2024.

 [17]  Yixuan Wu, Yizhou Wang, Shixiang Tang, Wenhao Wu, Tong He, Wanli Ouyang, Jian Wu, and Philip Torr. Dettoolchain: A new prompting paradigm to unleash detection ability of mllm. arXiv preprint arXiv:2403.12488 , 2024.

 [18]  Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection with multimodal large language models.  arXiv preprint arXiv:2305.18279 , 2023.

 [19]  Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions.  Scientific data , 5(1): 1C9, 2018.  

[20]  Kai Jin, Xingru Huang, Jingxing Zhou, Yunxiang Li, Yan Yan, Yibao Sun, Qianni Zhang, Yaqi Wang, and Juan Ye. Fives: A fundus image dataset for artificial intelligence based vessel segmentation.  Scientific Data , 9(1):475, 2022.

 [21]  Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In  Proceedings of the AAAI conference on artificial intelligence , volume 33, pages 590C597, 2019.

 [22]  Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge.  IEEE transactions on medical imaging , 38(2):550C560, 2018.

 [23]  Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In  Proceedings of the European conference on computer vision (ECCV) , pages 456C473, 2018.

 [24]  Yi Yang and Shawn Newsam. Bag-of-visual-words and spatial extensions for land-use clas- sification. In  Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems , pages 270C279, 2010.

 [25]  Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.  IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing , 12(7):2217C2226, 2019.

 [26]  Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In  2012 IEEE conference on computer vision and pattern recognition , pages 3498C3505. IEEE, 2012.

 [27]  Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 3606C3613, 2014.

 [28]  Yuanfeng Ji, Lu Zhang, Jiaxiang Wu, Bingzhe Wu, Long-Kai Huang, Tingyang Xu, Yu Rong, Lanqing Li, Jie Ren, Ding Xue, et al. Drugood: Out-of-distribution (ood) dataset curator and benchmark for ai-aided drug discoveryCa focus on affinity prediction problems with noise annotations.  arXiv preprint arXiv:2201.09637 , 2022.

 [29]  Wei-Lin Chen, Cheng-Kuang Wu, and Hsin-Hsi Chen. Self-icl: Zero-shot in-context learning with self-generated demonstrations.  arXiv preprint arXiv:2305.15035 , 2023.

 [30]  Introducing meta llama 3: The most capable openly available llm to date. URL  https: //ai.meta.com/blog/meta-llama-3/ .  

# A Prompts used for ICL experiments  

# A.1 Prompt used for image classification experiments  

prompt   $=$   "" for demo in demo_examples: prompt   $+=$   f"""  $<<\tt I M G>>G$  iven the image above, answer the following question- using the specified format. Question: What is in the image above? Choices: {str(class_desp)} Answer Choice: {demo.answer} """  

prompt   $+=$   f"""<<IMG  $>>$  Given the image above, answer the following question- using the specified format. Question: What is in the image above? Choices: {str(class_desp)}  

Please respond with the following format: ---BEGIN FORMAT TEMPLATE---Answer Choice: [Your Answer Choice Here] Confidence Score: [Your Numerical Prediction Confidence Score Here From 0 To 1] ---END FORMAT TEMPLATE---  

Do not deviate from the above format. Repeat the format template for the answer."""  

# A.2 Prompts used for image classification experiments with batching  

prompt   $=$   "" for demo in demo_examples: prompt   $+=$   f"""<<IMG>>Given the image above, answer the following question- using the specified format. Question: What is in the image above? Choices: {str(class_desp)} Answer Choice: {demo[1]}  

for idx, i in enumerate(test_df.iloc[start_idx:end_idx].itertuples()): prompt   $+=$   f"""<<IMG>>Given the image above, answer the following question- using the specified format. Question {qn_idx}: What is in the image above? Choices {qn_idx}: {str(class_desp)}  

"""  

for i in range(start_idx, end_idx): qn_idx   $=$   i-start_idx  $+1$  prompt   $+=$   f""" Please respond with the following format for each question: ---BEGIN FORMAT TEMPLATE FOR QUESTION {qn_idx}---Answer Choice {qn_idx}: [Your Answer Choice Here for Question {qn_idx}] Confidence Score {qn_idx}: [Your Numerical Prediction Confidence Score Here- From 0 To 1 for Question {qn_idx}] ---END FORMAT TEMPLATE FOR QUESTION {qn_idx}---  

Do not deviate from the above format. Repeat the format template for the answer."""  

# A.3 Prompts used for batching ablation experiments  

# A.3.1 Prefixing images  

prompt   $=$   "" for demo in prefix image paths: prompt   $+=$   f"""<<IMG>> """ prompt   $+=$   "Above are some images from the same dataset. " qns_idx  $\begin{array}{r l}{\mathbf{\Sigma}=}&{{}\left[\begin{array}{l l l}\end{array}\right]}\end{array}$  for idx, i in enumerate(test_df.iloc[start_idx:end_idx].itertuples()): qn_idx   $=$   idx  $^{+1}$  prompt   $+=$   f"""<<IMG>> Given the image above, answer the following question- using the specified format. Question {qn_idx}: What is in the image above? Choices {qn_idx}: {str(class_desp)} """ for i in range(start_idx, end_idx): qn_idx   $=$   i-start_idx  $+1$  prompt   $+=$   f""" Please respond with the following format for each question: ---BEGIN FORMAT TEMPLATE FOR QUESTION {qn_idx}---Answer Choice {qn_idx}: [Your Answer Choice Here for Question {qn_idx}] Confidence Score {qn_idx}: [Your Numerical Prediction Confidence Score Here- From 0 To 1 for Question {qn_idx}] ---END FORMAT TEMPLATE FOR QUESTION {qn_idx}---  

Do not deviate from the above format. Repeat the format template for the answer."""  

# B Prompt selection  

We utilize a different set of prompts to test the robustness of ManyICL to differences in prompt wording. We randomly sample two datasets (HAM10000 and EuroSAT) for this experiment due to budget limit.  

# B.1 Prompts used for prompt selection experiments  

Note that only the question section is shown here, and prompt 1 is used for all other image classifica- tion experiments.  

# B.1.1 Prompt 1  

<<IMG>>Given the image above, answer the following question using the specified format. Question {qn_idx}: What is in the image above? Choices {qn_idx}: {str(class_desp)}  

# B.1.2 Prompt 2  

<<IMG>>Given the image above, answer the following question using the specified format. Question {qn_idx}: Which class does this image belong to? Choices {qn_idx}: {str(class_desp)}  

# B.1.3 Prompt 3  

Question {qn_idx}: <<IMG>>Classify the image above, choose from {str(class_desp)}  

![](images/99b88c369400230a2a9184c28d29b1a0a33e58585e37cb97682df3efb971c013.jpg)  
Figure 5:  Sensitivity analysis of many-shot ICL.  These plots show the change in task performance on two datasets as the number of demonstrating examples increases, using three different prompts. For all experiments on sensitivity analysis, the Gemini 1.5 Pro model is used. The  $x$  -axis is in the logarithmic scale, representing the number of demonstrating examples plus one. The log-linear improvement until the optimal performance is consistent across all prompts selected.  

# B.2 Prompt selection results  

Figure 5 shows the sensitivity of performance to prompt selection on two datasets with three prompts. While there exists a small deviation in performance, but the overall log-linear improvement trend is consistent.  

# C GPT4(V)-Turbo performance under many-shot ICL  

GPT4(V)-Turbo shows mixed results for many-shot ICL, with substantial performance improvements on HAM1000, UCMerced, EuroSAT, and DTD, but minimal improvements or no improvement across the other six datasets (Figure 6). However, we note that we were unable to increase the number of demo examples to the same level as Gemini 1.5 Pro because GPT4(V)-Turbo has a shorter context window and is more prone to timeout errors when scaling. Additionally, GPT4(V)-Turbo seems to generally underperform Gemini   $1.5\;\mathrm{Pro}$   across the datasets excluding FIVES and EuroSAT for which it seems to mostly match the Gemini 1.5 Pro performance. GPT4(V)-Turbo performance on DrugOOD Assay shows high variance, resembling that of Gemini 1.5 Pro with the peak performance at 40 demo examples.  

# D Performance of many-shot ICL on medical QA tasks  

# D.1 Prompt used for medical QA experiments (MedQA, MedMCQA)  

prompt   $=$   "You are an expert in answering medical exam questions. " for demo in demo_examples: prompt   $+=$   f"""Question: {demo.question} Choices: {demo.options} Answer: {demo.answer}  

prompt   $+=$   f"""Question: {actual.question} Choices: {actual.options}  

![](images/7fbb1117324c771ddde9abff02886943aa7c4fe74670ff3b2eecb97784e88c53.jpg)  
Figure 6:  GPT4(V)-Turbo and GPT-4o performance from zero-shot to many-shot ICL.  X-axis is in log scale.  

![](images/7fa5e3c838b281a806b1b13a640b9052e8547e81867ca302096902f370c707f6.jpg)  
Figure 7: Many-shot ICL performances of medical QA tasks.  

Do not deviate from the above format. Repeat the format template for the answer."""  

# D.2 Results  

Figure 7 shows the results on medical QA tasks.  

# Acknowledgments and Disclosure of Funding  

We thank Dr. Jeff Dean, Yuhui Zhang, Dr. Mutallip Anwar, Kefan Dong, Rishi Bommasani, Ravi B. Sojitra, Chen Shani and Annie Chen for their feedback on the ideas and manuscript. Yixing Jiang is supported by National Science Scholarship (PhD). This work is also supported by Google cloud credit. Dr. Jonathan Chen has received research funding support in part by NIH/National Institute of Allergy and Infectious Diseases (1R01AI17812101), NIH/National Institute on Drug Abuse Clinical Trials Network (UG1DA015815 - CTN-0136), Gordon and Betty Moore Foundation (Grant #12409), Stanford Artificial Intelligence in Medicine and Imaging - Human-Centered Artificial Intelligence (AIMI-HAI) Partnership Grant, Google, Inc. Research collaboration Co-I to leverage EHR data to predict a range of clinical outcomes, American Heart Association - Strategically Focused Research Network - Diversity in Clinical Trials and NIH-NCATS-CTSA grant (UL1TR003142) for common research resources.  
2024-08-19 16:18:34,454 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1.md: 文件路径无效: # Many-Shot In-Context Learning in Multimodal Foundation Models  

# Yixing Jiang \u2217 Jeremy Irvin \u2217 Ji Hun Wang Muhammad Ahmed Chaudhry Jonathan H. Chen Andrew Y. Ng  

Stanford University {jiang6,jirvin16,jihunwang,ang}@cs.stanford.edu {mahmedch,jonc101}@stanford.edu  

# Abstract  

Large language models are well-known to be effective at few-shot in-context learning (ICL). Recent advancements in multimodal foundation models have en- abled unprecedentedly long context windows, presenting an opportunity to explore their capability to perform ICL with many more demonstrating examples. In this work, we evaluate the performance of multimodal foundation models scaling from few-shot to many-shot ICL. We benchmark GPT-4o and Gemini   $1.5\ \mathrm{Pr}\bar{\mathrm{o}}$  across 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification). We observe that many-shot ICL, including up to al- most 2,000 multimodal demonstrating examples, leads to substantial improvements compared to few-shot   $\langle<\!100$   examples) ICL across all of the datasets. Further, Gemini   $1.5~\mathrm{Pro}$   performance continues to improve log-linearly up to the max- imum number of tested examples on many datasets. Given the high inference costs associated with the long prompts required for many-shot ICL, we also ex- plore the impact of batching multiple queries in a single API call. We show that batching up to 50 queries can lead to performance improvements under zero-shot and manyCshot ICL, with substantial gains in the zero-shot setting on multiple datasets, while drastically reducing per-query cost and latency. Finally, we measure ICL data efficiency of the models, or the rate at which the models learn from more demonstrating examples. We find that while GPT-4o and Gemini   $1.5~\mathrm{Pro}$  achieve similar zero-shot performance across the datasets, Gemini 1.5 Pro exhibits higher ICL data efficiency than GPT-4o on most datasets. Our results suggest that many-shot ICL could enable users to efficiently adapt multimodal foundation models to new applications and domains. Our codebase is publicly available at https://github.com/stanfordmlgroup/ManyICL .  

# 1 Introduction  

Large language models (LLMs) have been shown to substantially benefit from the inclusion of a few demonstrating examples ( shots ) in the LLM context before the test query [ 1 C 3 ]. This phenomenon, commonly referred to as in-context learning (ICL), enables LLMs to learn from few shots without any updates to model parameters, and therefore improves specialization to new tasks without any further model training. More recently, large multimodal models (LMMs) have also demonstrated the capability of learning from in-context examples [4C6]. Han et al. [5] and Zhang et al. [6] both show that few-shot multimodal ICL specifically helps to improve LMM performance on out-domain or out-of-distribution tasks.  

![](images/c2295c09026975f63765663f5663420aa41ecabe03fed3062728054bfde7fa4a.jpg)  
Figure 1:  Many-shot multimodal in-context learning compared to zero-shot and few-shot multimodal ICL.  In zero-shot and few-shot settings, respectively, no demonstrating examples or only a small number of demonstrating examples are provided in the context before the test query. In a many-shot ICL setting, we include a large number of demonstrating examples in the prompt, whereas in batched many-shot ICL, we perform multiple queries at once using query references.  

While few-shot ICL has enabled promising performance improvements for both LLMs and LMMs, limited model context windows have constrained research on the impact of increasing the number of demonstrating examples on performance. This is especially true for LMMs as most use a large number of visual tokens to represent images. However, due to recent advancements enabling substantially longer context windows C for example, 128,000 tokens for GPT-4o and up to one million tokens for Gemini 1.5 Pro C it is now possible to explore the effect of drastically increasing the number of demonstrating examples.  

To investigate the capability of state-of-the-art multimodal foundation models to perform many-shot ICL, we conduct a large suite of experiments benchmarking model performance on 10 datasets spanning several domains and image classification tasks after scaling up the number of demonstrating examples by multiple orders of magnitude. Specifically, our contributions are as follows:  

1.  We show that providing multimodal foundation models with many demonstrating examples leads to substantial performance improvements compared to providing only a few demon- strating examples. We observe that the performance of Gemini 1.5 Pro generally improves log-linearly as the number of demonstrating examples increases, whereas GPT-4o exhibits less stable improvements as the number of in-context examples increases. 2.  We measure the data efficiency of the models under ICL as the number of demonstrating examples is increased, and find that Gemini  $1.5\;\mathrm{Pro}$   exhibits higher ICL data efficiency than GPT-4o on most datasets. 3.  We demonstrate that batching multiple queries into a single request can achieve similar or better performance than single query requests in a many-shot setting, while enabling substantially lower per-example latency and much cheaper per-example inference cost. 4.  We find that batching multiple questions can lead to substantial performance improvements in a zero-shot setting. We design experiments to explain this phenomenon, and find that the improvements are due to a combination of domain calibration, class calibration, and self-generated demonstrating examples due to autoregressive decoding.  

![](images/37c467c4191e2fac2d692a0fab388edb7e621823aa9ac0d60e897b50ce2cfee8.jpg)  

# 2 Related Work  

Scaling ICL.  The seminal work of Brown et al.  [1]  discovered performance improvements for LLMs from increasing the number of in-context examples, but the tested number of demonstrating examples was low (10 to 100), likely due to the restrictive context size (2048 tokens for GPT3). Increasing the number of in-context examples has only been explored recently by a few works [ 7 C 9 ]. Both Li et al.  [7]  and Agarwal et al.  [8]  explore scaling in-context learning to more than 1,000 demonstrating examples and find performance improvements across multiple tasks. However, their experiments are limited to text-only benchmarks and do not compare performance across different models.  

Multimodal ICL.  Due to the recent emergence of LMMs, research on multimodal ICL is still nascent. One prior work developed a new model to leverage complex prompts composed of multimodal inputs in order to allow models to compare images [ 10 ], while other recent works explored the general iz ability of GPT-4V and Gemini to multimodal out-domain and out-of-distribution tasks, and found that ICL leads to performance benefits for both models across many tasks [ 6 ,  5 ]. However, none of these works have leveraged the new largely expanded context windows to investigate the effects of increasing the number of demonstrating examples.  

Batch Querying.  Multiple prior works have explored batching queries (also commonly referred to as batch prompting) for more efficient and cheaper inference. Batch prompting was first introduced in Cheng et al.  [11] , leading to comparable or better performance than single prompting, while achieving substantially reduced inference token cost and latency. Lin et al.  [12]  observe performance degradation with batched prompts in longer contexts, and propose a variety of techniques to mitigate the performance loss. More recently, additional variations of batch prompting have been proposed, including grouping similar questions together [ 13 ], batching prompts of different tasks [ 14 ], and concatenating multiple images into a single image collage [ 15 ]. We again note that batch prompting with high numbers of demonstrating examples and high numbers of queries has only become feasible due to larger context windows of recent models.  

# 3 Methods  

We conduct several experiments to test the effect of increasing the number of demonstrating examples on the performance of two state-of-the-art multimodal foundation models: GPT-4o and Gemini 1.5 Pro (Section 3.1). We benchmark their performance using standard performance metrics as well as an ICL data efficiency metric (Section 3.3) on 10 datasets spanning several vision domains and image classification tasks (Section 3.2). We conduct ablation studies to test the impact of batching queries on model performance and explain the substantial improvement in zero-shot settings (Section 4.2). We refer to the many-shot in-context learning framework as many-shot ICL. Figure 1 provides an illustrative summary of many-shot ICL and batched many-shot ICL compared to zero-shot and few-shot ICL.  

# 3.1 Models  

We use three state-of-the-art multimodal foundation models with public API access, namely GPT-4o, GPT4(V)-Turbo [ 4 ], and Gemini 1.5 Pro [ 16 ]. Because GPT-4o performs substantially better than GPT4(V)-Turbo, we focus on the results of GPT-4o and Gemini 1.5 Pro in the main text, and include GPT4(V)-Turbo results in the Appendix. We do not utilize Claude3-Opus in our experiments, as it only accepts up to 20 images in one request at the time of writing. The specific endpoint for for GPT-4o is “gpt-4o-2024-05-13”, for GPT-4(V)-Turbo is “gpt-4-turbo-2024-04-09”, and for Gemini  $1.5\;\mathrm{Pro}$   is “gemini-1.5-pro-preview-  $.0409^{\ast}$  . We use the API service provided by OpenAI for GPT-4o and GPT-4(V)-Turbo, and the API service provided by Google Cloud on Vertex AI for Gemini 1.5 Pro. We set the temperature to zero for all models and a random seed for GPT-4(V)-Turbo and GPT-4o to obtain more deterministic responses. To prevent models from abstaining (which happens rarely), we rerun the query until an answer is provided.  

# 3.2 Datasets  

We benchmark the model performance on 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification). We choose to focus on image classification tasks as other tasks such as region captioning would require substantially more tokens thereby limiting the total number of demonstrating examples, and most LMMs are not yet capable of accurately producing localizations required for other tasks like bounding boxes and segmentation masks [ 17 ,  18 ]. Table 1 provides a summary of the datasets used in this study.  

For all datasets, we construct a set of demonstration (demo) examples from the original training and validation splits used for in-context learning and a test set from the original test split (if one exists) to evaluate the performance of the models. We randomly sample the demo and test sets from the original dataset without replacement. For the multi-class and fine-grained classification datasets, we perform a class-stratified sampling, ensuring an equal number of examples per class in both the demo and test sets. For the multi-label classification dataset (CheXpert), we sample an equal number of positive and negative samples per class in both the demo and test sets. We note that, since the task is multi-label, this sampling procedure does not result in an exactly equal number of examples per class. The per-dataset sizes of the full demo and test sets are shown in Table 1, and we increase the number of demonstration examples up to the numbers shown in the table while ensuring class balance for the scaling experiments.  

# 3.3 Evaluation Metrics  

We use standard metrics to evaluate model performance on each dataset. Specifically, we measure performance using accuracy for all multi-class classification datasets as they are sampled to have a balanced class distribution. For multi-label classification on CheXpert, we use the macro-averaged F1 metric. In the rare case of parsing errors, we consider the response as incorrect. To estimate the variability around the evaluation metrics, we compute standard deviation using bootstrapping with 1,000 bootstrap replicates.  

In addition to standard performance metrics, we measure the data efficiency of each model. Specifi- cally, we compute a linear regression between  $\log_{10}(N+1)$   (with    $N$   the number of examples) and model performance, enforcing that the line passes through the zero-shot performance point. This value approximates the amount of performance improvement from zero-shot expected from including an order of magnitude more demonstrating examples.  

Table 1:  Summary of benchmark datasets.  We use 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification).  

![](images/7205245d4d93aea120e20581767010eac80e677a2b8fa68813b14b64b9b19c77.jpg)  

![Table 2:  Many-shot ICL performance and efficiency comparison.  We report the performance under a zero-shot regime and performance at the optimal demo set size as well as the many-shot ICL data efficiency of GPT-4o and Gemini  $1.5~\mathrm{Pro}$  . We measure performance using accuracy on all datasets except CheXpert, for which we use macro-average F1. We bold the highest ICL data efficiency between the two models on each dataset. ](images/5747989fbf490aa5015d41f600c67f18638cc9a72f5f650cee24797078ec72b4.jpg)  

# 4 Results  

We present many-shot ICL performance using batched queries in Section 4.1, investigate the impact of batching queries on performance in Section 4.2, and provide an analysis on cost and latency in Section 4.3. Results using GPT4(V)-Turbo are in Appendix C.  

# 4.1 Increasing number of demonstrating examples  

Main Results.  Gemini 1.5 Pro exhibits consistent and substantial improvements as the number of demonstrating examples increases across all datasets except for DrugOOD Assay (Figure 2). Gemini  $1.5\;\mathrm{Pro}$   shows particularly large improvements from many-shot ICL on HAM10000 (  ${\it+23\%}$   accuracy compared to zero-shot,  $+16\%$   compared to 7 examples), FIVES   ${\it\Omega}+29\%$   compared to zero-shot,  $+27\%$  compared to 20 examples), and EuroSAT   $(+38\%$   compared to zero-shot,  $+31\%$   compared to 10 examples). Notably, for 5 out of the 10 datasets (FIVES, UCMerced, EuroSAT, Oxford Pets, and DTD), Gemini 1.5 Pro performance continues to improve up to the highest number of demonstrating examples considered   $(\mathord{\sim}1,\!000$   examples). On the other 5 datasets, the optimal performance occurs prior to the highest number of demo examples, with the maximum number of demo examples leading to similar or slightly worse performance than the optimal demo set size. On the other hand, Gemini 1.5 Pro performance on DrugOOD Assay does not substantially benefit from many-shot ICL, with high variance in performance across demo sizes and the peak performance at 40 demo examples.  

Similarly, GPT-4o shows substantial performance improvements on all datasets except FIVES and DrugOOD Assay using many-shot ICL, but the improvement is not consistent. For many datasets, performance drops sharply at first and then improves significantly as the number of demonstrating examples increases further, resulting in V-shaped scaling curves (Figure 2). We also note that we were unable to increase the number of demo examples to the same level as considered for Gemini 1.5 Pro because GPT-4o has a shorter context window and is more prone to timeout errors with longer inputs. GPT-4o performance on DrugOOD Assay shows high variance, similar to Gemini  $1.5\;\mathrm{Pro}$  , with the peak performance observed at 50 demo examples.  

Sensitivity to prompt selection.  We also explore a different set of prompts to test the robustness of many-shot ICL to differences in prompt wording on two datasets. While there is a small deviation in performance between different prompts, the overall log-linear improvement trend is consistent across the prompts. Details can be found in Appendix B.  

ICL data efficiency.  We find Gemini   $1.5\;\mathrm{Pro}$   demonstrates higher ICL data efficiency than GPT- 4o across all datasets except TerraIncognita and DTD (Table 2). Gemini 1.5 Pro ICL efficiency is especially high on EuroSAT, with  $20.61\%$   improvement in accuracy for every   $10\mathbf{x}$   more demo examples, and lowest on DrugOOD Assay (2.03), Camelyon17 (3.00), and TerraIncognita (3.50). GPT-4o ICL data efficiency is especially high on TerraIncognita   $(20.50\%)$   and EuroSat (19.40).  

![](images/bff391610e9c220cf73f2868432fb8d09d38554b2955e4c0cc37c21cee960108.jpg)  
Figure 3:  Gemini 1.5 Pro performance under many-shot and zero-shot ICL when varying the amount of queries included in every request.  We show performance per batch size with the optimal number of demo examples (many-shot) and no demo examples (zero-shot). The  $x$  -axis is in log scale. Under the many-shot regime, batching queries leads to no substantial drop in performance compared to individual queries when we choose a suitable batch size. For zero-shot, including only one query is suboptimal for many datasets.  

Gemini 1.5 Pro has a positive efficiency on all datasets and GPT-4o has a positive data efficiency on 9 of the 10 datasets (excluding Oxford Pets). Importantly, both models benefit substantially from many-shot ICL at the optimal demo set size, with an average improvement of  $+17\%$   for both Gemini 1.5 Pro and GPT-4o.  

# 4.2 Impact of batching queries  

As including a large set of demo examples in the prompt leads to much longer sequence lengths and therefore higher inference time and cost, we consider batching queries in a single prompt to reduce per-query cost, and examine the impact of different batch sizes on model performance. Due to its superior performance and free preview access, we use Gemini   $1.5\;\mathrm{Pro}$   for these experiments.  

Main Results.  We find minimal performance degradations, and sometimes performance improve- ments, as we increase the number of queries included in each batch across under both zero-shot and many-shot (at the optimal demo set size) regimes (Figure 3). Notably, using a single query each time with many-shot ICL is suboptimal across many of the datasets. We find that the optimal batch size is among the three largest sizes on every dataset except CheXpert and EuroSAT, which both see optimal performance with a single query at a time.  

We additionally observe that including a single query at a time is suboptimal on most datasets in the zero-shot regime. Surprisingly, performance with the highest batch size is substantially higher across three datasets under the zero-shot regime, with a consistent performance improvement as the batch size is increased on both UCMerced and Terraincognita.  

Zero-shot performance improvements from batching queries.  We conduct several additional experiments to investigate why batch querying can lead to large performance improvements under the zero-shot regime on TerraIncognita and UCMerced. We hypothesize that this improvement may be due to three potential benefits from ICL: (1) domain calibration, where the model benefits from seeing more images in the domain in order to adapt to it, (2) class calibration, where seeing images of different classes enables the model to better calibrate its outputs, and (3) self-ICL (shown to be effective in prior work [ 29 ]), where the model can learn from self-generated demonstrations due to autoregressive decoding. We design experiments to isolate the potential benefits from each of these types of ICL between asking a single query to batching 50 queries together.  

First, to measure potential improvement from domain calibration, we include 49 images from the same class in the prompt without including any label. We find a  $3.0\%$   improvement on TerraIncognita  

![](images/d448fc079ac55d9535c79b6dcb9cd01f084aad34cad1d391283da4d8103570e9.jpg)  
Figure 4:  Ablation study to investigate why batching queries leads to performance improve- ments when using Gemini  $\mathbf{1.5\,Pr0}$   in a zero-shot setting.  The first bar shows performance when including a single query, the second adds 49 unlabeled images from a single class, the third adds 49 unlabeled images in total from all classes, the fourth adds model responses to include self-generated demonstrations, and the last includes 50 queries in one request.  

and  $2.6\%$   degradation on UCMerced, suggesting domain calibration is helpful for the former but not the latter. Second, to capture performance gains from class calibration, we include a random sample of 49 images in the prompt, again without including the label. We see a further  $3.5\%$   improvement on TerraIncognita (  $6.5\%$   improvement from a single query) and a  $4.5\%$   improvement from a single query on UCMerced, suggesting including the context of class-balanced images is helpful even without labels. Third, to capture additional performance improvements from the self-generated labels, we obtain predicted labels from the zero-shot model using a single query for each of the 49 randomly sampled images and add them to the prompt. We observe further performance increase on both datasets, with  $5.5\%$   on TerraIncognita and  $2.7\%$   on UCMerced. The final total accuracy is similar to asking the 50 questions each round, which suggests these three components mostly explain the reason for improved zero-shot performance under a larger query batch size.  

# 4.3 Cost and latency analysis  

Many-shot ICL incurs zero additional training cost, but per-query inference can be costly and slow due to long input contexts. To quantitatively measure this, we compute the latency and cost associated with the zero-shot and many-shot requests with and without batching when using Gemini   $1.5\;\mathrm{Pro}$   on HAM10000 and TerraIncognita. We calculate the costs using the Gemini 1.5 Pro preview pricing (  $\S7$   per 1 million input tokens and   $\S21$   per 1 million output tokens). For fair comparison and to minimize data transfer artifacts, all requests are sent to the same location where the VM instance is held (“us-central1”). We run the query three times under each setting and report the average.  

In the zero-shot regime, we see substantial per-example latency reductions due to query batching, close to a  $10\mathbf{x}$   reduction on HAM10000 and  $2\mathbf{X}$   on TerraIncognita (Table 3). The per-example cost is similar between the two as there is no additional context needed for including demonstrating examples. In the many-shot regime, we observe substantial reductions in both per-example latency and cost on both datasets. Specifically, for HAM10000, we find a near  $35\mathrm{x}$   reduction in latency and  $10\mathbf{x}$   reduction in cost, and   $20\mathbf{x}$   reduction in latency and  $45\mathbf{x}$   reduction in cost for TerraIncognita.  

# 5 Discussion  

In this study, we evaluate many-shot ICL of state-of-the-art multimodal foundation models across 10 datasets and find consistent performance improvements across most of the datasets. Batching queries with many-shot ICL further exhibits substantially reduced per-example latency and inference costs without compromising performance.  

Our findings suggest that these multimodal foundation models have the capability of performing ICL with large numbers of demonstrating examples, which may have significant implications on their practical use. For example, it was previously impossible to adapt these large, private models to  

![Table 3:  Inference latency and cost using Gemini 1.5 Pro with and without query batching.  We use 50 queries per batch. In the zero-shot setting, we can achieve lower per-example latency with batching, but the per-example cost remains identical. In the many-shot setting, the per-example cost and per-example latency both drop substantially with query batching. ](images/08cad9838361c7c5255cbdae6c004feabab7f6d3d3cb985b2b076061aef4f1a1.jpg)  

new tasks and domains, but many-shot ICL would enable users to leverage demonstrating examples to adapt the models. One significant advantage of many-shot ICL is its ability to get quick results even on the same day of model release, and that’s why we can finish our evaluation using GPT-4o within days. Furthermore, fine-tuning open-source models is the standard practice when practitioners have access to moderately sized datasets, but many-shot ICL may remove the need for fine-tuning, making it much easier to develop customized approaches. We note that it remains to be seen how traditional fine-tuning of these models compares to many-shot ICL with foundation models in terms of absolute performance and data efficiency, so future work should explore this. In addition, it is important to study general issues which plague those foundation models, such as hallucinations and biases, under the context of many-shot ICL and batching queries. For example, it would be interesting to explore if carefully curated and large sets of demonstrating examples can reduce biases across different sub-groups. We leave this to future work.  

Our study has limitations. First, we only explore performance under many-shot ICL on image classification tasks and with private foundation models. We believe these are the most practically relevant and common multimodal settings, but it is worthwhile for future work to explore potential benefits from many-shot ICL on other tasks and with upcoming open-source multimodal foundation models like LLaMA-3 [ 30 ]. Second, even after recent developments to increase context size, the size prohibits many-shot ICL from being used on datasets with a large number (several hundred or more) of classes. We anticipate that context window sizes will continue to increase in size over time which will mitigate this issue. Third, the datasets which were used to train these private models have not been disclosed, so it is difficult to tell whether the models have been trained on the datasets we selected. We argue that zero-shot performance across the datasets is far from perfect which provides evidence that the datasets have not been used for training, but we cannot determine that with certainty.  

# 6 Conclusion  

In summary, we show that multimodal foundation models are capable of many-shot ICL. We believe that these results pave a promising path forward to improve the adaptability and accessibility of large multimodal foundation models.  

# References  

[1]  Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.  

[2]  Archit Parnami and Minwoo Lee. Learning from few examples: A summary of approaches to few-shot learning.  arXiv preprint arXiv:2203.04291 , 2022.  

[3]  Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples: A survey on few-shot learning.  ACM computing surveys (csur) , 53(3):1C34, 2020.

 [4]  Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.  arXiv preprint arXiv:2303.08774 , 2023.

 [5]  Zhongyi Han, Guanglin Zhou, Rundong He, Jindong Wang, Xing Xie, Tailin Wu, Yilong Yin, Salman Khan, Lina Yao, Tongliang Liu, et al. How well does gpt-4v (ision) adapt to distribution shifts? a preliminary investigation.  arXiv preprint arXiv:2312.07424 , 2023.

 [6]  Xingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia Hai, Renzhe Xu, Yuqing Yang, Shikai Guan, Jiazheng Xu, and Peng Cui. On the out-of-distribution generalization of multimodal large language models.  arXiv preprint arXiv:2402.06599 , 2024.

 [7]  Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, Zhiyong Wu, and Lingpeng Kong. In-context learning with many demonstration examples.  arXiv preprint arXiv:2302.04931 , 2023.

 [8]  Rishabh Agarwal, Avi Singh, Lei M Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John D Co-Reyes, Eric Chu, et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018 , 2024.

 [9]  Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R Gormley, and Graham Neubig. In-context learning with long-context models: An in-depth exploration.  arXiv preprint arXiv:2405.00200 , 2024.

 [10]  Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with multi-modal in-context learning.  arXiv preprint arXiv:2309.07915 , 2023.

 [11]  Zhoujun Cheng, Jungo Kasai, and Tao Yu. Batch prompting: Efficient inference with large language model apis.  arXiv preprint arXiv:2301.08721 , 2023.

 [12]  Jianzhe Lin, Maurice Diesendruck, Liang Du, and Robin Abraham. Batchprompt: Accomplish more with less.  arXiv preprint arXiv:2309.00384 , 2023.

 [13]  Jiayi Liu, Tinghan Yang, and Jennifer Neville. Cliqueparcel: An approach for batching llm prompts that jointly optimizes efficiency and faithfulness.  arXiv preprint arXiv:2402.14833 , 2024.

 [14]  Guijin Son, Sangwon Baek, Sangdae Nam, Ilgyun Jeong, and Seungone Kim. Multi-task inference: Can large language models follow multiple instructions at once?  arXiv preprint arXiv:2402.11597 , 2024.

 [15]  Siyu Xu, Yunke Wang, Daochang Liu, and Chang Xu. Collage prompting: Budget-friendly visual recognition with gpt-4v.  arXiv preprint arXiv:2403.11468 , 2024.

 [16]  Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean- baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.  arXiv preprint arXiv:2403.05530 , 2024.

 [17]  Yixuan Wu, Yizhou Wang, Shixiang Tang, Wenhao Wu, Tong He, Wanli Ouyang, Jian Wu, and Philip Torr. Dettoolchain: A new prompting paradigm to unleash detection ability of mllm. arXiv preprint arXiv:2403.12488 , 2024.

 [18]  Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection with multimodal large language models.  arXiv preprint arXiv:2305.18279 , 2023.

 [19]  Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions.  Scientific data , 5(1): 1C9, 2018.  

[20]  Kai Jin, Xingru Huang, Jingxing Zhou, Yunxiang Li, Yan Yan, Yibao Sun, Qianni Zhang, Yaqi Wang, and Juan Ye. Fives: A fundus image dataset for artificial intelligence based vessel segmentation.  Scientific Data , 9(1):475, 2022.

 [21]  Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In  Proceedings of the AAAI conference on artificial intelligence , volume 33, pages 590C597, 2019.

 [22]  Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge.  IEEE transactions on medical imaging , 38(2):550C560, 2018.

 [23]  Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In  Proceedings of the European conference on computer vision (ECCV) , pages 456C473, 2018.

 [24]  Yi Yang and Shawn Newsam. Bag-of-visual-words and spatial extensions for land-use clas- sification. In  Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems , pages 270C279, 2010.

 [25]  Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.  IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing , 12(7):2217C2226, 2019.

 [26]  Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In  2012 IEEE conference on computer vision and pattern recognition , pages 3498C3505. IEEE, 2012.

 [27]  Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 3606C3613, 2014.

 [28]  Yuanfeng Ji, Lu Zhang, Jiaxiang Wu, Bingzhe Wu, Long-Kai Huang, Tingyang Xu, Yu Rong, Lanqing Li, Jie Ren, Ding Xue, et al. Drugood: Out-of-distribution (ood) dataset curator and benchmark for ai-aided drug discoveryCa focus on affinity prediction problems with noise annotations.  arXiv preprint arXiv:2201.09637 , 2022.

 [29]  Wei-Lin Chen, Cheng-Kuang Wu, and Hsin-Hsi Chen. Self-icl: Zero-shot in-context learning with self-generated demonstrations.  arXiv preprint arXiv:2305.15035 , 2023.

 [30]  Introducing meta llama 3: The most capable openly available llm to date. URL  https: //ai.meta.com/blog/meta-llama-3/ .  

# A Prompts used for ICL experiments  

# A.1 Prompt used for image classification experiments  

prompt   $=$   "" for demo in demo_examples: prompt   $+=$   f"""  $<<\tt I M G>>G$  iven the image above, answer the following question- using the specified format. Question: What is in the image above? Choices: {str(class_desp)} Answer Choice: {demo.answer} """  

prompt   $+=$   f"""<<IMG  $>>$  Given the image above, answer the following question- using the specified format. Question: What is in the image above? Choices: {str(class_desp)}  

Please respond with the following format: ---BEGIN FORMAT TEMPLATE---Answer Choice: [Your Answer Choice Here] Confidence Score: [Your Numerical Prediction Confidence Score Here From 0 To 1] ---END FORMAT TEMPLATE---  

Do not deviate from the above format. Repeat the format template for the answer."""  

# A.2 Prompts used for image classification experiments with batching  

prompt   $=$   "" for demo in demo_examples: prompt   $+=$   f"""<<IMG>>Given the image above, answer the following question- using the specified format. Question: What is in the image above? Choices: {str(class_desp)} Answer Choice: {demo[1]}  

for idx, i in enumerate(test_df.iloc[start_idx:end_idx].itertuples()): prompt   $+=$   f"""<<IMG>>Given the image above, answer the following question- using the specified format. Question {qn_idx}: What is in the image above? Choices {qn_idx}: {str(class_desp)}  

"""  

for i in range(start_idx, end_idx): qn_idx   $=$   i-start_idx  $+1$  prompt   $+=$   f""" Please respond with the following format for each question: ---BEGIN FORMAT TEMPLATE FOR QUESTION {qn_idx}---Answer Choice {qn_idx}: [Your Answer Choice Here for Question {qn_idx}] Confidence Score {qn_idx}: [Your Numerical Prediction Confidence Score Here- From 0 To 1 for Question {qn_idx}] ---END FORMAT TEMPLATE FOR QUESTION {qn_idx}---  

Do not deviate from the above format. Repeat the format template for the answer."""  

# A.3 Prompts used for batching ablation experiments  

# A.3.1 Prefixing images  

prompt   $=$   "" for demo in prefix image paths: prompt   $+=$   f"""<<IMG>> """ prompt   $+=$   "Above are some images from the same dataset. " qns_idx  $\begin{array}{r l}{\mathbf{\Sigma}=}&{{}\left[\begin{array}{l l l}\end{array}\right]}\end{array}$  for idx, i in enumerate(test_df.iloc[start_idx:end_idx].itertuples()): qn_idx   $=$   idx  $^{+1}$  prompt   $+=$   f"""<<IMG>> Given the image above, answer the following question- using the specified format. Question {qn_idx}: What is in the image above? Choices {qn_idx}: {str(class_desp)} """ for i in range(start_idx, end_idx): qn_idx   $=$   i-start_idx  $+1$  prompt   $+=$   f""" Please respond with the following format for each question: ---BEGIN FORMAT TEMPLATE FOR QUESTION {qn_idx}---Answer Choice {qn_idx}: [Your Answer Choice Here for Question {qn_idx}] Confidence Score {qn_idx}: [Your Numerical Prediction Confidence Score Here- From 0 To 1 for Question {qn_idx}] ---END FORMAT TEMPLATE FOR QUESTION {qn_idx}---  

Do not deviate from the above format. Repeat the format template for the answer."""  

# B Prompt selection  

We utilize a different set of prompts to test the robustness of ManyICL to differences in prompt wording. We randomly sample two datasets (HAM10000 and EuroSAT) for this experiment due to budget limit.  

# B.1 Prompts used for prompt selection experiments  

Note that only the question section is shown here, and prompt 1 is used for all other image classifica- tion experiments.  

# B.1.1 Prompt 1  

<<IMG>>Given the image above, answer the following question using the specified format. Question {qn_idx}: What is in the image above? Choices {qn_idx}: {str(class_desp)}  

# B.1.2 Prompt 2  

<<IMG>>Given the image above, answer the following question using the specified format. Question {qn_idx}: Which class does this image belong to? Choices {qn_idx}: {str(class_desp)}  

# B.1.3 Prompt 3  

Question {qn_idx}: <<IMG>>Classify the image above, choose from {str(class_desp)}  

![](images/99b88c369400230a2a9184c28d29b1a0a33e58585e37cb97682df3efb971c013.jpg)  
Figure 5:  Sensitivity analysis of many-shot ICL.  These plots show the change in task performance on two datasets as the number of demonstrating examples increases, using three different prompts. For all experiments on sensitivity analysis, the Gemini 1.5 Pro model is used. The  $x$  -axis is in the logarithmic scale, representing the number of demonstrating examples plus one. The log-linear improvement until the optimal performance is consistent across all prompts selected.  

# B.2 Prompt selection results  

Figure 5 shows the sensitivity of performance to prompt selection on two datasets with three prompts. While there exists a small deviation in performance, but the overall log-linear improvement trend is consistent.  

# C GPT4(V)-Turbo performance under many-shot ICL  

GPT4(V)-Turbo shows mixed results for many-shot ICL, with substantial performance improvements on HAM1000, UCMerced, EuroSAT, and DTD, but minimal improvements or no improvement across the other six datasets (Figure 6). However, we note that we were unable to increase the number of demo examples to the same level as Gemini 1.5 Pro because GPT4(V)-Turbo has a shorter context window and is more prone to timeout errors when scaling. Additionally, GPT4(V)-Turbo seems to generally underperform Gemini   $1.5\;\mathrm{Pro}$   across the datasets excluding FIVES and EuroSAT for which it seems to mostly match the Gemini 1.5 Pro performance. GPT4(V)-Turbo performance on DrugOOD Assay shows high variance, resembling that of Gemini 1.5 Pro with the peak performance at 40 demo examples.  

# D Performance of many-shot ICL on medical QA tasks  

# D.1 Prompt used for medical QA experiments (MedQA, MedMCQA)  

prompt   $=$   "You are an expert in answering medical exam questions. " for demo in demo_examples: prompt   $+=$   f"""Question: {demo.question} Choices: {demo.options} Answer: {demo.answer}  

prompt   $+=$   f"""Question: {actual.question} Choices: {actual.options}  

![](images/7fbb1117324c771ddde9abff02886943aa7c4fe74670ff3b2eecb97784e88c53.jpg)  
Figure 6:  GPT4(V)-Turbo and GPT-4o performance from zero-shot to many-shot ICL.  X-axis is in log scale.  

![](images/7fa5e3c838b281a806b1b13a640b9052e8547e81867ca302096902f370c707f6.jpg)  
Figure 7: Many-shot ICL performances of medical QA tasks.  

Do not deviate from the above format. Repeat the format template for the answer."""  

# D.2 Results  

Figure 7 shows the results on medical QA tasks.  

# Acknowledgments and Disclosure of Funding  

We thank Dr. Jeff Dean, Yuhui Zhang, Dr. Mutallip Anwar, Kefan Dong, Rishi Bommasani, Ravi B. Sojitra, Chen Shani and Annie Chen for their feedback on the ideas and manuscript. Yixing Jiang is supported by National Science Scholarship (PhD). This work is also supported by Google cloud credit. Dr. Jonathan Chen has received research funding support in part by NIH/National Institute of Allergy and Infectious Diseases (1R01AI17812101), NIH/National Institute on Drug Abuse Clinical Trials Network (UG1DA015815 - CTN-0136), Gordon and Betty Moore Foundation (Grant #12409), Stanford Artificial Intelligence in Medicine and Imaging - Human-Centered Artificial Intelligence (AIMI-HAI) Partnership Grant, Google, Inc. Research collaboration Co-I to leverage EHR data to predict a range of clinical outcomes, American Heart Association - Strategically Focused Research Network - Diversity in Clinical Trials and NIH-NCATS-CTSA grant (UL1TR003142) for common research resources.  
2024-08-19 16:21:26,334 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1.md: 文件路径无效: # Many-Shot In-Context Learning in Multimodal Foundation Models  

# Yixing Jiang \u2217 Jeremy Irvin \u2217 Ji Hun Wang Muhammad Ahmed Chaudhry Jonathan H. Chen Andrew Y. Ng  

Stanford University {jiang6,jirvin16,jihunwang,ang}@cs.stanford.edu {mahmedch,jonc101}@stanford.edu  

# Abstract  

Large language models are well-known to be effective at few-shot in-context learning (ICL). Recent advancements in multimodal foundation models have en- abled unprecedentedly long context windows, presenting an opportunity to explore their capability to perform ICL with many more demonstrating examples. In this work, we evaluate the performance of multimodal foundation models scaling from few-shot to many-shot ICL. We benchmark GPT-4o and Gemini   $1.5\ \mathrm{Pr}\bar{\mathrm{o}}$  across 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification). We observe that many-shot ICL, including up to al- most 2,000 multimodal demonstrating examples, leads to substantial improvements compared to few-shot   $\langle<\!100$   examples) ICL across all of the datasets. Further, Gemini   $1.5~\mathrm{Pro}$   performance continues to improve log-linearly up to the max- imum number of tested examples on many datasets. Given the high inference costs associated with the long prompts required for many-shot ICL, we also ex- plore the impact of batching multiple queries in a single API call. We show that batching up to 50 queries can lead to performance improvements under zero-shot and manyCshot ICL, with substantial gains in the zero-shot setting on multiple datasets, while drastically reducing per-query cost and latency. Finally, we measure ICL data efficiency of the models, or the rate at which the models learn from more demonstrating examples. We find that while GPT-4o and Gemini   $1.5~\mathrm{Pro}$  achieve similar zero-shot performance across the datasets, Gemini 1.5 Pro exhibits higher ICL data efficiency than GPT-4o on most datasets. Our results suggest that many-shot ICL could enable users to efficiently adapt multimodal foundation models to new applications and domains. Our codebase is publicly available at https://github.com/stanfordmlgroup/ManyICL .  

# 1 Introduction  

Large language models (LLMs) have been shown to substantially benefit from the inclusion of a few demonstrating examples ( shots ) in the LLM context before the test query [ 1 C 3 ]. This phenomenon, commonly referred to as in-context learning (ICL), enables LLMs to learn from few shots without any updates to model parameters, and therefore improves specialization to new tasks without any further model training. More recently, large multimodal models (LMMs) have also demonstrated the capability of learning from in-context examples [4C6]. Han et al. [5] and Zhang et al. [6] both show that few-shot multimodal ICL specifically helps to improve LMM performance on out-domain or out-of-distribution tasks.  

![](images/c2295c09026975f63765663f5663420aa41ecabe03fed3062728054bfde7fa4a.jpg)  
Figure 1:  Many-shot multimodal in-context learning compared to zero-shot and few-shot multimodal ICL.  In zero-shot and few-shot settings, respectively, no demonstrating examples or only a small number of demonstrating examples are provided in the context before the test query. In a many-shot ICL setting, we include a large number of demonstrating examples in the prompt, whereas in batched many-shot ICL, we perform multiple queries at once using query references.  

While few-shot ICL has enabled promising performance improvements for both LLMs and LMMs, limited model context windows have constrained research on the impact of increasing the number of demonstrating examples on performance. This is especially true for LMMs as most use a large number of visual tokens to represent images. However, due to recent advancements enabling substantially longer context windows C for example, 128,000 tokens for GPT-4o and up to one million tokens for Gemini 1.5 Pro C it is now possible to explore the effect of drastically increasing the number of demonstrating examples.  

To investigate the capability of state-of-the-art multimodal foundation models to perform many-shot ICL, we conduct a large suite of experiments benchmarking model performance on 10 datasets spanning several domains and image classification tasks after scaling up the number of demonstrating examples by multiple orders of magnitude. Specifically, our contributions are as follows:  

1.  We show that providing multimodal foundation models with many demonstrating examples leads to substantial performance improvements compared to providing only a few demon- strating examples. We observe that the performance of Gemini 1.5 Pro generally improves log-linearly as the number of demonstrating examples increases, whereas GPT-4o exhibits less stable improvements as the number of in-context examples increases. 2.  We measure the data efficiency of the models under ICL as the number of demonstrating examples is increased, and find that Gemini  $1.5\;\mathrm{Pro}$   exhibits higher ICL data efficiency than GPT-4o on most datasets. 3.  We demonstrate that batching multiple queries into a single request can achieve similar or better performance than single query requests in a many-shot setting, while enabling substantially lower per-example latency and much cheaper per-example inference cost. 4.  We find that batching multiple questions can lead to substantial performance improvements in a zero-shot setting. We design experiments to explain this phenomenon, and find that the improvements are due to a combination of domain calibration, class calibration, and self-generated demonstrating examples due to autoregressive decoding.  

![](images/37c467c4191e2fac2d692a0fab388edb7e621823aa9ac0d60e897b50ce2cfee8.jpg)  

# 2 Related Work  

Scaling ICL.  The seminal work of Brown et al.  [1]  discovered performance improvements for LLMs from increasing the number of in-context examples, but the tested number of demonstrating examples was low (10 to 100), likely due to the restrictive context size (2048 tokens for GPT3). Increasing the number of in-context examples has only been explored recently by a few works [ 7 C 9 ]. Both Li et al.  [7]  and Agarwal et al.  [8]  explore scaling in-context learning to more than 1,000 demonstrating examples and find performance improvements across multiple tasks. However, their experiments are limited to text-only benchmarks and do not compare performance across different models.  

Multimodal ICL.  Due to the recent emergence of LMMs, research on multimodal ICL is still nascent. One prior work developed a new model to leverage complex prompts composed of multimodal inputs in order to allow models to compare images [ 10 ], while other recent works explored the general iz ability of GPT-4V and Gemini to multimodal out-domain and out-of-distribution tasks, and found that ICL leads to performance benefits for both models across many tasks [ 6 ,  5 ]. However, none of these works have leveraged the new largely expanded context windows to investigate the effects of increasing the number of demonstrating examples.  

Batch Querying.  Multiple prior works have explored batching queries (also commonly referred to as batch prompting) for more efficient and cheaper inference. Batch prompting was first introduced in Cheng et al.  [11] , leading to comparable or better performance than single prompting, while achieving substantially reduced inference token cost and latency. Lin et al.  [12]  observe performance degradation with batched prompts in longer contexts, and propose a variety of techniques to mitigate the performance loss. More recently, additional variations of batch prompting have been proposed, including grouping similar questions together [ 13 ], batching prompts of different tasks [ 14 ], and concatenating multiple images into a single image collage [ 15 ]. We again note that batch prompting with high numbers of demonstrating examples and high numbers of queries has only become feasible due to larger context windows of recent models.  

# 3 Methods  

We conduct several experiments to test the effect of increasing the number of demonstrating examples on the performance of two state-of-the-art multimodal foundation models: GPT-4o and Gemini 1.5 Pro (Section 3.1). We benchmark their performance using standard performance metrics as well as an ICL data efficiency metric (Section 3.3) on 10 datasets spanning several vision domains and image classification tasks (Section 3.2). We conduct ablation studies to test the impact of batching queries on model performance and explain the substantial improvement in zero-shot settings (Section 4.2). We refer to the many-shot in-context learning framework as many-shot ICL. Figure 1 provides an illustrative summary of many-shot ICL and batched many-shot ICL compared to zero-shot and few-shot ICL.  

# 3.1 Models  

We use three state-of-the-art multimodal foundation models with public API access, namely GPT-4o, GPT4(V)-Turbo [ 4 ], and Gemini 1.5 Pro [ 16 ]. Because GPT-4o performs substantially better than GPT4(V)-Turbo, we focus on the results of GPT-4o and Gemini 1.5 Pro in the main text, and include GPT4(V)-Turbo results in the Appendix. We do not utilize Claude3-Opus in our experiments, as it only accepts up to 20 images in one request at the time of writing. The specific endpoint for for GPT-4o is “gpt-4o-2024-05-13”, for GPT-4(V)-Turbo is “gpt-4-turbo-2024-04-09”, and for Gemini  $1.5\;\mathrm{Pro}$   is “gemini-1.5-pro-preview-  $.0409^{\ast}$  . We use the API service provided by OpenAI for GPT-4o and GPT-4(V)-Turbo, and the API service provided by Google Cloud on Vertex AI for Gemini 1.5 Pro. We set the temperature to zero for all models and a random seed for GPT-4(V)-Turbo and GPT-4o to obtain more deterministic responses. To prevent models from abstaining (which happens rarely), we rerun the query until an answer is provided.  

# 3.2 Datasets  

We benchmark the model performance on 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification). We choose to focus on image classification tasks as other tasks such as region captioning would require substantially more tokens thereby limiting the total number of demonstrating examples, and most LMMs are not yet capable of accurately producing localizations required for other tasks like bounding boxes and segmentation masks [ 17 ,  18 ]. Table 1 provides a summary of the datasets used in this study.  

For all datasets, we construct a set of demonstration (demo) examples from the original training and validation splits used for in-context learning and a test set from the original test split (if one exists) to evaluate the performance of the models. We randomly sample the demo and test sets from the original dataset without replacement. For the multi-class and fine-grained classification datasets, we perform a class-stratified sampling, ensuring an equal number of examples per class in both the demo and test sets. For the multi-label classification dataset (CheXpert), we sample an equal number of positive and negative samples per class in both the demo and test sets. We note that, since the task is multi-label, this sampling procedure does not result in an exactly equal number of examples per class. The per-dataset sizes of the full demo and test sets are shown in Table 1, and we increase the number of demonstration examples up to the numbers shown in the table while ensuring class balance for the scaling experiments.  

# 3.3 Evaluation Metrics  

We use standard metrics to evaluate model performance on each dataset. Specifically, we measure performance using accuracy for all multi-class classification datasets as they are sampled to have a balanced class distribution. For multi-label classification on CheXpert, we use the macro-averaged F1 metric. In the rare case of parsing errors, we consider the response as incorrect. To estimate the variability around the evaluation metrics, we compute standard deviation using bootstrapping with 1,000 bootstrap replicates.  

In addition to standard performance metrics, we measure the data efficiency of each model. Specifi- cally, we compute a linear regression between  $\log_{10}(N+1)$   (with    $N$   the number of examples) and model performance, enforcing that the line passes through the zero-shot performance point. This value approximates the amount of performance improvement from zero-shot expected from including an order of magnitude more demonstrating examples.  

Table 1:  Summary of benchmark datasets.  We use 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification).  

![](images/7205245d4d93aea120e20581767010eac80e677a2b8fa68813b14b64b9b19c77.jpg)  

![Table 2:  Many-shot ICL performance and efficiency comparison.  We report the performance under a zero-shot regime and performance at the optimal demo set size as well as the many-shot ICL data efficiency of GPT-4o and Gemini  $1.5~\mathrm{Pro}$  . We measure performance using accuracy on all datasets except CheXpert, for which we use macro-average F1. We bold the highest ICL data efficiency between the two models on each dataset. ](images/5747989fbf490aa5015d41f600c67f18638cc9a72f5f650cee24797078ec72b4.jpg)  

# 4 Results  

We present many-shot ICL performance using batched queries in Section 4.1, investigate the impact of batching queries on performance in Section 4.2, and provide an analysis on cost and latency in Section 4.3. Results using GPT4(V)-Turbo are in Appendix C.  

# 4.1 Increasing number of demonstrating examples  

Main Results.  Gemini 1.5 Pro exhibits consistent and substantial improvements as the number of demonstrating examples increases across all datasets except for DrugOOD Assay (Figure 2). Gemini  $1.5\;\mathrm{Pro}$   shows particularly large improvements from many-shot ICL on HAM10000 (  ${\it+23\%}$   accuracy compared to zero-shot,  $+16\%$   compared to 7 examples), FIVES   ${\it\Omega}+29\%$   compared to zero-shot,  $+27\%$  compared to 20 examples), and EuroSAT   $(+38\%$   compared to zero-shot,  $+31\%$   compared to 10 examples). Notably, for 5 out of the 10 datasets (FIVES, UCMerced, EuroSAT, Oxford Pets, and DTD), Gemini 1.5 Pro performance continues to improve up to the highest number of demonstrating examples considered   $(\mathord{\sim}1,\!000$   examples). On the other 5 datasets, the optimal performance occurs prior to the highest number of demo examples, with the maximum number of demo examples leading to similar or slightly worse performance than the optimal demo set size. On the other hand, Gemini 1.5 Pro performance on DrugOOD Assay does not substantially benefit from many-shot ICL, with high variance in performance across demo sizes and the peak performance at 40 demo examples.  

Similarly, GPT-4o shows substantial performance improvements on all datasets except FIVES and DrugOOD Assay using many-shot ICL, but the improvement is not consistent. For many datasets, performance drops sharply at first and then improves significantly as the number of demonstrating examples increases further, resulting in V-shaped scaling curves (Figure 2). We also note that we were unable to increase the number of demo examples to the same level as considered for Gemini 1.5 Pro because GPT-4o has a shorter context window and is more prone to timeout errors with longer inputs. GPT-4o performance on DrugOOD Assay shows high variance, similar to Gemini  $1.5\;\mathrm{Pro}$  , with the peak performance observed at 50 demo examples.  

Sensitivity to prompt selection.  We also explore a different set of prompts to test the robustness of many-shot ICL to differences in prompt wording on two datasets. While there is a small deviation in performance between different prompts, the overall log-linear improvement trend is consistent across the prompts. Details can be found in Appendix B.  

ICL data efficiency.  We find Gemini   $1.5\;\mathrm{Pro}$   demonstrates higher ICL data efficiency than GPT- 4o across all datasets except TerraIncognita and DTD (Table 2). Gemini 1.5 Pro ICL efficiency is especially high on EuroSAT, with  $20.61\%$   improvement in accuracy for every   $10\mathbf{x}$   more demo examples, and lowest on DrugOOD Assay (2.03), Camelyon17 (3.00), and TerraIncognita (3.50). GPT-4o ICL data efficiency is especially high on TerraIncognita   $(20.50\%)$   and EuroSat (19.40).  

![](images/bff391610e9c220cf73f2868432fb8d09d38554b2955e4c0cc37c21cee960108.jpg)  
Figure 3:  Gemini 1.5 Pro performance under many-shot and zero-shot ICL when varying the amount of queries included in every request.  We show performance per batch size with the optimal number of demo examples (many-shot) and no demo examples (zero-shot). The  $x$  -axis is in log scale. Under the many-shot regime, batching queries leads to no substantial drop in performance compared to individual queries when we choose a suitable batch size. For zero-shot, including only one query is suboptimal for many datasets.  

Gemini 1.5 Pro has a positive efficiency on all datasets and GPT-4o has a positive data efficiency on 9 of the 10 datasets (excluding Oxford Pets). Importantly, both models benefit substantially from many-shot ICL at the optimal demo set size, with an average improvement of  $+17\%$   for both Gemini 1.5 Pro and GPT-4o.  

# 4.2 Impact of batching queries  

As including a large set of demo examples in the prompt leads to much longer sequence lengths and therefore higher inference time and cost, we consider batching queries in a single prompt to reduce per-query cost, and examine the impact of different batch sizes on model performance. Due to its superior performance and free preview access, we use Gemini   $1.5\;\mathrm{Pro}$   for these experiments.  

Main Results.  We find minimal performance degradations, and sometimes performance improve- ments, as we increase the number of queries included in each batch across under both zero-shot and many-shot (at the optimal demo set size) regimes (Figure 3). Notably, using a single query each time with many-shot ICL is suboptimal across many of the datasets. We find that the optimal batch size is among the three largest sizes on every dataset except CheXpert and EuroSAT, which both see optimal performance with a single query at a time.  

We additionally observe that including a single query at a time is suboptimal on most datasets in the zero-shot regime. Surprisingly, performance with the highest batch size is substantially higher across three datasets under the zero-shot regime, with a consistent performance improvement as the batch size is increased on both UCMerced and Terraincognita.  

Zero-shot performance improvements from batching queries.  We conduct several additional experiments to investigate why batch querying can lead to large performance improvements under the zero-shot regime on TerraIncognita and UCMerced. We hypothesize that this improvement may be due to three potential benefits from ICL: (1) domain calibration, where the model benefits from seeing more images in the domain in order to adapt to it, (2) class calibration, where seeing images of different classes enables the model to better calibrate its outputs, and (3) self-ICL (shown to be effective in prior work [ 29 ]), where the model can learn from self-generated demonstrations due to autoregressive decoding. We design experiments to isolate the potential benefits from each of these types of ICL between asking a single query to batching 50 queries together.  

First, to measure potential improvement from domain calibration, we include 49 images from the same class in the prompt without including any label. We find a  $3.0\%$   improvement on TerraIncognita  

![](images/d448fc079ac55d9535c79b6dcb9cd01f084aad34cad1d391283da4d8103570e9.jpg)  
Figure 4:  Ablation study to investigate why batching queries leads to performance improve- ments when using Gemini  $\mathbf{1.5\,Pr0}$   in a zero-shot setting.  The first bar shows performance when including a single query, the second adds 49 unlabeled images from a single class, the third adds 49 unlabeled images in total from all classes, the fourth adds model responses to include self-generated demonstrations, and the last includes 50 queries in one request.  

and  $2.6\%$   degradation on UCMerced, suggesting domain calibration is helpful for the former but not the latter. Second, to capture performance gains from class calibration, we include a random sample of 49 images in the prompt, again without including the label. We see a further  $3.5\%$   improvement on TerraIncognita (  $6.5\%$   improvement from a single query) and a  $4.5\%$   improvement from a single query on UCMerced, suggesting including the context of class-balanced images is helpful even without labels. Third, to capture additional performance improvements from the self-generated labels, we obtain predicted labels from the zero-shot model using a single query for each of the 49 randomly sampled images and add them to the prompt. We observe further performance increase on both datasets, with  $5.5\%$   on TerraIncognita and  $2.7\%$   on UCMerced. The final total accuracy is similar to asking the 50 questions each round, which suggests these three components mostly explain the reason for improved zero-shot performance under a larger query batch size.  

# 4.3 Cost and latency analysis  

Many-shot ICL incurs zero additional training cost, but per-query inference can be costly and slow due to long input contexts. To quantitatively measure this, we compute the latency and cost associated with the zero-shot and many-shot requests with and without batching when using Gemini   $1.5\;\mathrm{Pro}$   on HAM10000 and TerraIncognita. We calculate the costs using the Gemini 1.5 Pro preview pricing (  $\S7$   per 1 million input tokens and   $\S21$   per 1 million output tokens). For fair comparison and to minimize data transfer artifacts, all requests are sent to the same location where the VM instance is held (“us-central1”). We run the query three times under each setting and report the average.  

In the zero-shot regime, we see substantial per-example latency reductions due to query batching, close to a  $10\mathbf{x}$   reduction on HAM10000 and  $2\mathbf{X}$   on TerraIncognita (Table 3). The per-example cost is similar between the two as there is no additional context needed for including demonstrating examples. In the many-shot regime, we observe substantial reductions in both per-example latency and cost on both datasets. Specifically, for HAM10000, we find a near  $35\mathrm{x}$   reduction in latency and  $10\mathbf{x}$   reduction in cost, and   $20\mathbf{x}$   reduction in latency and  $45\mathbf{x}$   reduction in cost for TerraIncognita.  

# 5 Discussion  

In this study, we evaluate many-shot ICL of state-of-the-art multimodal foundation models across 10 datasets and find consistent performance improvements across most of the datasets. Batching queries with many-shot ICL further exhibits substantially reduced per-example latency and inference costs without compromising performance.  

Our findings suggest that these multimodal foundation models have the capability of performing ICL with large numbers of demonstrating examples, which may have significant implications on their practical use. For example, it was previously impossible to adapt these large, private models to  

![Table 3:  Inference latency and cost using Gemini 1.5 Pro with and without query batching.  We use 50 queries per batch. In the zero-shot setting, we can achieve lower per-example latency with batching, but the per-example cost remains identical. In the many-shot setting, the per-example cost and per-example latency both drop substantially with query batching. ](images/08cad9838361c7c5255cbdae6c004feabab7f6d3d3cb985b2b076061aef4f1a1.jpg)  

new tasks and domains, but many-shot ICL would enable users to leverage demonstrating examples to adapt the models. One significant advantage of many-shot ICL is its ability to get quick results even on the same day of model release, and that’s why we can finish our evaluation using GPT-4o within days. Furthermore, fine-tuning open-source models is the standard practice when practitioners have access to moderately sized datasets, but many-shot ICL may remove the need for fine-tuning, making it much easier to develop customized approaches. We note that it remains to be seen how traditional fine-tuning of these models compares to many-shot ICL with foundation models in terms of absolute performance and data efficiency, so future work should explore this. In addition, it is important to study general issues which plague those foundation models, such as hallucinations and biases, under the context of many-shot ICL and batching queries. For example, it would be interesting to explore if carefully curated and large sets of demonstrating examples can reduce biases across different sub-groups. We leave this to future work.  

Our study has limitations. First, we only explore performance under many-shot ICL on image classification tasks and with private foundation models. We believe these are the most practically relevant and common multimodal settings, but it is worthwhile for future work to explore potential benefits from many-shot ICL on other tasks and with upcoming open-source multimodal foundation models like LLaMA-3 [ 30 ]. Second, even after recent developments to increase context size, the size prohibits many-shot ICL from being used on datasets with a large number (several hundred or more) of classes. We anticipate that context window sizes will continue to increase in size over time which will mitigate this issue. Third, the datasets which were used to train these private models have not been disclosed, so it is difficult to tell whether the models have been trained on the datasets we selected. We argue that zero-shot performance across the datasets is far from perfect which provides evidence that the datasets have not been used for training, but we cannot determine that with certainty.  

# 6 Conclusion  

In summary, we show that multimodal foundation models are capable of many-shot ICL. We believe that these results pave a promising path forward to improve the adaptability and accessibility of large multimodal foundation models.  

# References  

[1]  Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.  

[2]  Archit Parnami and Minwoo Lee. Learning from few examples: A summary of approaches to few-shot learning.  arXiv preprint arXiv:2203.04291 , 2022.  

[3]  Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples: A survey on few-shot learning.  ACM computing surveys (csur) , 53(3):1C34, 2020.

 [4]  Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.  arXiv preprint arXiv:2303.08774 , 2023.

 [5]  Zhongyi Han, Guanglin Zhou, Rundong He, Jindong Wang, Xing Xie, Tailin Wu, Yilong Yin, Salman Khan, Lina Yao, Tongliang Liu, et al. How well does gpt-4v (ision) adapt to distribution shifts? a preliminary investigation.  arXiv preprint arXiv:2312.07424 , 2023.

 [6]  Xingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia Hai, Renzhe Xu, Yuqing Yang, Shikai Guan, Jiazheng Xu, and Peng Cui. On the out-of-distribution generalization of multimodal large language models.  arXiv preprint arXiv:2402.06599 , 2024.

 [7]  Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, Zhiyong Wu, and Lingpeng Kong. In-context learning with many demonstration examples.  arXiv preprint arXiv:2302.04931 , 2023.

 [8]  Rishabh Agarwal, Avi Singh, Lei M Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John D Co-Reyes, Eric Chu, et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018 , 2024.

 [9]  Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R Gormley, and Graham Neubig. In-context learning with long-context models: An in-depth exploration.  arXiv preprint arXiv:2405.00200 , 2024.

 [10]  Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with multi-modal in-context learning.  arXiv preprint arXiv:2309.07915 , 2023.

 [11]  Zhoujun Cheng, Jungo Kasai, and Tao Yu. Batch prompting: Efficient inference with large language model apis.  arXiv preprint arXiv:2301.08721 , 2023.

 [12]  Jianzhe Lin, Maurice Diesendruck, Liang Du, and Robin Abraham. Batchprompt: Accomplish more with less.  arXiv preprint arXiv:2309.00384 , 2023.

 [13]  Jiayi Liu, Tinghan Yang, and Jennifer Neville. Cliqueparcel: An approach for batching llm prompts that jointly optimizes efficiency and faithfulness.  arXiv preprint arXiv:2402.14833 , 2024.

 [14]  Guijin Son, Sangwon Baek, Sangdae Nam, Ilgyun Jeong, and Seungone Kim. Multi-task inference: Can large language models follow multiple instructions at once?  arXiv preprint arXiv:2402.11597 , 2024.

 [15]  Siyu Xu, Yunke Wang, Daochang Liu, and Chang Xu. Collage prompting: Budget-friendly visual recognition with gpt-4v.  arXiv preprint arXiv:2403.11468 , 2024.

 [16]  Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean- baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.  arXiv preprint arXiv:2403.05530 , 2024.

 [17]  Yixuan Wu, Yizhou Wang, Shixiang Tang, Wenhao Wu, Tong He, Wanli Ouyang, Jian Wu, and Philip Torr. Dettoolchain: A new prompting paradigm to unleash detection ability of mllm. arXiv preprint arXiv:2403.12488 , 2024.

 [18]  Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection with multimodal large language models.  arXiv preprint arXiv:2305.18279 , 2023.

 [19]  Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions.  Scientific data , 5(1): 1C9, 2018.  

[20]  Kai Jin, Xingru Huang, Jingxing Zhou, Yunxiang Li, Yan Yan, Yibao Sun, Qianni Zhang, Yaqi Wang, and Juan Ye. Fives: A fundus image dataset for artificial intelligence based vessel segmentation.  Scientific Data , 9(1):475, 2022.

 [21]  Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In  Proceedings of the AAAI conference on artificial intelligence , volume 33, pages 590C597, 2019.

 [22]  Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge.  IEEE transactions on medical imaging , 38(2):550C560, 2018.

 [23]  Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In  Proceedings of the European conference on computer vision (ECCV) , pages 456C473, 2018.

 [24]  Yi Yang and Shawn Newsam. Bag-of-visual-words and spatial extensions for land-use clas- sification. In  Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems , pages 270C279, 2010.

 [25]  Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.  IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing , 12(7):2217C2226, 2019.

 [26]  Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In  2012 IEEE conference on computer vision and pattern recognition , pages 3498C3505. IEEE, 2012.

 [27]  Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 3606C3613, 2014.

 [28]  Yuanfeng Ji, Lu Zhang, Jiaxiang Wu, Bingzhe Wu, Long-Kai Huang, Tingyang Xu, Yu Rong, Lanqing Li, Jie Ren, Ding Xue, et al. Drugood: Out-of-distribution (ood) dataset curator and benchmark for ai-aided drug discoveryCa focus on affinity prediction problems with noise annotations.  arXiv preprint arXiv:2201.09637 , 2022.

 [29]  Wei-Lin Chen, Cheng-Kuang Wu, and Hsin-Hsi Chen. Self-icl: Zero-shot in-context learning with self-generated demonstrations.  arXiv preprint arXiv:2305.15035 , 2023.

 [30]  Introducing meta llama 3: The most capable openly available llm to date. URL  https: //ai.meta.com/blog/meta-llama-3/ .  

# A Prompts used for ICL experiments  

# A.1 Prompt used for image classification experiments  

prompt   $=$   "" for demo in demo_examples: prompt   $+=$   f"""  $<<\tt I M G>>G$  iven the image above, answer the following question- using the specified format. Question: What is in the image above? Choices: {str(class_desp)} Answer Choice: {demo.answer} """  

prompt   $+=$   f"""<<IMG  $>>$  Given the image above, answer the following question- using the specified format. Question: What is in the image above? Choices: {str(class_desp)}  

Please respond with the following format: ---BEGIN FORMAT TEMPLATE---Answer Choice: [Your Answer Choice Here] Confidence Score: [Your Numerical Prediction Confidence Score Here From 0 To 1] ---END FORMAT TEMPLATE---  

Do not deviate from the above format. Repeat the format template for the answer."""  

# A.2 Prompts used for image classification experiments with batching  

prompt   $=$   "" for demo in demo_examples: prompt   $+=$   f"""<<IMG>>Given the image above, answer the following question- using the specified format. Question: What is in the image above? Choices: {str(class_desp)} Answer Choice: {demo[1]}  

for idx, i in enumerate(test_df.iloc[start_idx:end_idx].itertuples()): prompt   $+=$   f"""<<IMG>>Given the image above, answer the following question- using the specified format. Question {qn_idx}: What is in the image above? Choices {qn_idx}: {str(class_desp)}  

"""  

for i in range(start_idx, end_idx): qn_idx   $=$   i-start_idx  $+1$  prompt   $+=$   f""" Please respond with the following format for each question: ---BEGIN FORMAT TEMPLATE FOR QUESTION {qn_idx}---Answer Choice {qn_idx}: [Your Answer Choice Here for Question {qn_idx}] Confidence Score {qn_idx}: [Your Numerical Prediction Confidence Score Here- From 0 To 1 for Question {qn_idx}] ---END FORMAT TEMPLATE FOR QUESTION {qn_idx}---  

Do not deviate from the above format. Repeat the format template for the answer."""  

# A.3 Prompts used for batching ablation experiments  

# A.3.1 Prefixing images  

prompt   $=$   "" for demo in prefix image paths: prompt   $+=$   f"""<<IMG>> """ prompt   $+=$   "Above are some images from the same dataset. " qns_idx  $\begin{array}{r l}{\mathbf{\Sigma}=}&{{}\left[\begin{array}{l l l}\end{array}\right]}\end{array}$  for idx, i in enumerate(test_df.iloc[start_idx:end_idx].itertuples()): qn_idx   $=$   idx  $^{+1}$  prompt   $+=$   f"""<<IMG>> Given the image above, answer the following question- using the specified format. Question {qn_idx}: What is in the image above? Choices {qn_idx}: {str(class_desp)} """ for i in range(start_idx, end_idx): qn_idx   $=$   i-start_idx  $+1$  prompt   $+=$   f""" Please respond with the following format for each question: ---BEGIN FORMAT TEMPLATE FOR QUESTION {qn_idx}---Answer Choice {qn_idx}: [Your Answer Choice Here for Question {qn_idx}] Confidence Score {qn_idx}: [Your Numerical Prediction Confidence Score Here- From 0 To 1 for Question {qn_idx}] ---END FORMAT TEMPLATE FOR QUESTION {qn_idx}---  

Do not deviate from the above format. Repeat the format template for the answer."""  

# B Prompt selection  

We utilize a different set of prompts to test the robustness of ManyICL to differences in prompt wording. We randomly sample two datasets (HAM10000 and EuroSAT) for this experiment due to budget limit.  

# B.1 Prompts used for prompt selection experiments  

Note that only the question section is shown here, and prompt 1 is used for all other image classifica- tion experiments.  

# B.1.1 Prompt 1  

<<IMG>>Given the image above, answer the following question using the specified format. Question {qn_idx}: What is in the image above? Choices {qn_idx}: {str(class_desp)}  

# B.1.2 Prompt 2  

<<IMG>>Given the image above, answer the following question using the specified format. Question {qn_idx}: Which class does this image belong to? Choices {qn_idx}: {str(class_desp)}  

# B.1.3 Prompt 3  

Question {qn_idx}: <<IMG>>Classify the image above, choose from {str(class_desp)}  

![](images/99b88c369400230a2a9184c28d29b1a0a33e58585e37cb97682df3efb971c013.jpg)  
Figure 5:  Sensitivity analysis of many-shot ICL.  These plots show the change in task performance on two datasets as the number of demonstrating examples increases, using three different prompts. For all experiments on sensitivity analysis, the Gemini 1.5 Pro model is used. The  $x$  -axis is in the logarithmic scale, representing the number of demonstrating examples plus one. The log-linear improvement until the optimal performance is consistent across all prompts selected.  

# B.2 Prompt selection results  

Figure 5 shows the sensitivity of performance to prompt selection on two datasets with three prompts. While there exists a small deviation in performance, but the overall log-linear improvement trend is consistent.  

# C GPT4(V)-Turbo performance under many-shot ICL  

GPT4(V)-Turbo shows mixed results for many-shot ICL, with substantial performance improvements on HAM1000, UCMerced, EuroSAT, and DTD, but minimal improvements or no improvement across the other six datasets (Figure 6). However, we note that we were unable to increase the number of demo examples to the same level as Gemini 1.5 Pro because GPT4(V)-Turbo has a shorter context window and is more prone to timeout errors when scaling. Additionally, GPT4(V)-Turbo seems to generally underperform Gemini   $1.5\;\mathrm{Pro}$   across the datasets excluding FIVES and EuroSAT for which it seems to mostly match the Gemini 1.5 Pro performance. GPT4(V)-Turbo performance on DrugOOD Assay shows high variance, resembling that of Gemini 1.5 Pro with the peak performance at 40 demo examples.  

# D Performance of many-shot ICL on medical QA tasks  

# D.1 Prompt used for medical QA experiments (MedQA, MedMCQA)  

prompt   $=$   "You are an expert in answering medical exam questions. " for demo in demo_examples: prompt   $+=$   f"""Question: {demo.question} Choices: {demo.options} Answer: {demo.answer}  

prompt   $+=$   f"""Question: {actual.question} Choices: {actual.options}  

![](images/7fbb1117324c771ddde9abff02886943aa7c4fe74670ff3b2eecb97784e88c53.jpg)  
Figure 6:  GPT4(V)-Turbo and GPT-4o performance from zero-shot to many-shot ICL.  X-axis is in log scale.  

![](images/7fa5e3c838b281a806b1b13a640b9052e8547e81867ca302096902f370c707f6.jpg)  
Figure 7: Many-shot ICL performances of medical QA tasks.  

Do not deviate from the above format. Repeat the format template for the answer."""  

# D.2 Results  

Figure 7 shows the results on medical QA tasks.  

# Acknowledgments and Disclosure of Funding  

We thank Dr. Jeff Dean, Yuhui Zhang, Dr. Mutallip Anwar, Kefan Dong, Rishi Bommasani, Ravi B. Sojitra, Chen Shani and Annie Chen for their feedback on the ideas and manuscript. Yixing Jiang is supported by National Science Scholarship (PhD). This work is also supported by Google cloud credit. Dr. Jonathan Chen has received research funding support in part by NIH/National Institute of Allergy and Infectious Diseases (1R01AI17812101), NIH/National Institute on Drug Abuse Clinical Trials Network (UG1DA015815 - CTN-0136), Gordon and Betty Moore Foundation (Grant #12409), Stanford Artificial Intelligence in Medicine and Imaging - Human-Centered Artificial Intelligence (AIMI-HAI) Partnership Grant, Google, Inc. Research collaboration Co-I to leverage EHR data to predict a range of clinical outcomes, American Heart Association - Strategically Focused Research Network - Diversity in Clinical Trials and NIH-NCATS-CTSA grant (UL1TR003142) for common research resources.  
2024-08-19 16:21:56,188 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1.md: 文件路径无效: # Many-Shot In-Context Learning in Multimodal Foundation Models  

# Yixing Jiang \u2217 Jeremy Irvin \u2217 Ji Hun Wang Muhammad Ahmed Chaudhry Jonathan H. Chen Andrew Y. Ng  

Stanford University {jiang6,jirvin16,jihunwang,ang}@cs.stanford.edu {mahmedch,jonc101}@stanford.edu  

# Abstract  

Large language models are well-known to be effective at few-shot in-context learning (ICL). Recent advancements in multimodal foundation models have en- abled unprecedentedly long context windows, presenting an opportunity to explore their capability to perform ICL with many more demonstrating examples. In this work, we evaluate the performance of multimodal foundation models scaling from few-shot to many-shot ICL. We benchmark GPT-4o and Gemini   $1.5\ \mathrm{Pr}\bar{\mathrm{o}}$  across 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification). We observe that many-shot ICL, including up to al- most 2,000 multimodal demonstrating examples, leads to substantial improvements compared to few-shot   $\langle<\!100$   examples) ICL across all of the datasets. Further, Gemini   $1.5~\mathrm{Pro}$   performance continues to improve log-linearly up to the max- imum number of tested examples on many datasets. Given the high inference costs associated with the long prompts required for many-shot ICL, we also ex- plore the impact of batching multiple queries in a single API call. We show that batching up to 50 queries can lead to performance improvements under zero-shot and manyCshot ICL, with substantial gains in the zero-shot setting on multiple datasets, while drastically reducing per-query cost and latency. Finally, we measure ICL data efficiency of the models, or the rate at which the models learn from more demonstrating examples. We find that while GPT-4o and Gemini   $1.5~\mathrm{Pro}$  achieve similar zero-shot performance across the datasets, Gemini 1.5 Pro exhibits higher ICL data efficiency than GPT-4o on most datasets. Our results suggest that many-shot ICL could enable users to efficiently adapt multimodal foundation models to new applications and domains. Our codebase is publicly available at https://github.com/stanfordmlgroup/ManyICL .  

# 1 Introduction  

Large language models (LLMs) have been shown to substantially benefit from the inclusion of a few demonstrating examples ( shots ) in the LLM context before the test query [ 1 C 3 ]. This phenomenon, commonly referred to as in-context learning (ICL), enables LLMs to learn from few shots without any updates to model parameters, and therefore improves specialization to new tasks without any further model training. More recently, large multimodal models (LMMs) have also demonstrated the capability of learning from in-context examples [4C6]. Han et al. [5] and Zhang et al. [6] both show that few-shot multimodal ICL specifically helps to improve LMM performance on out-domain or out-of-distribution tasks.  

![](images/c2295c09026975f63765663f5663420aa41ecabe03fed3062728054bfde7fa4a.jpg)  
Figure 1:  Many-shot multimodal in-context learning compared to zero-shot and few-shot multimodal ICL.  In zero-shot and few-shot settings, respectively, no demonstrating examples or only a small number of demonstrating examples are provided in the context before the test query. In a many-shot ICL setting, we include a large number of demonstrating examples in the prompt, whereas in batched many-shot ICL, we perform multiple queries at once using query references.  

While few-shot ICL has enabled promising performance improvements for both LLMs and LMMs, limited model context windows have constrained research on the impact of increasing the number of demonstrating examples on performance. This is especially true for LMMs as most use a large number of visual tokens to represent images. However, due to recent advancements enabling substantially longer context windows C for example, 128,000 tokens for GPT-4o and up to one million tokens for Gemini 1.5 Pro C it is now possible to explore the effect of drastically increasing the number of demonstrating examples.  

To investigate the capability of state-of-the-art multimodal foundation models to perform many-shot ICL, we conduct a large suite of experiments benchmarking model performance on 10 datasets spanning several domains and image classification tasks after scaling up the number of demonstrating examples by multiple orders of magnitude. Specifically, our contributions are as follows:  

1.  We show that providing multimodal foundation models with many demonstrating examples leads to substantial performance improvements compared to providing only a few demon- strating examples. We observe that the performance of Gemini 1.5 Pro generally improves log-linearly as the number of demonstrating examples increases, whereas GPT-4o exhibits less stable improvements as the number of in-context examples increases. 2.  We measure the data efficiency of the models under ICL as the number of demonstrating examples is increased, and find that Gemini  $1.5\;\mathrm{Pro}$   exhibits higher ICL data efficiency than GPT-4o on most datasets. 3.  We demonstrate that batching multiple queries into a single request can achieve similar or better performance than single query requests in a many-shot setting, while enabling substantially lower per-example latency and much cheaper per-example inference cost. 4.  We find that batching multiple questions can lead to substantial performance improvements in a zero-shot setting. We design experiments to explain this phenomenon, and find that the improvements are due to a combination of domain calibration, class calibration, and self-generated demonstrating examples due to autoregressive decoding.  

![](images/37c467c4191e2fac2d692a0fab388edb7e621823aa9ac0d60e897b50ce2cfee8.jpg)  

# 2 Related Work  

Scaling ICL.  The seminal work of Brown et al.  [1]  discovered performance improvements for LLMs from increasing the number of in-context examples, but the tested number of demonstrating examples was low (10 to 100), likely due to the restrictive context size (2048 tokens for GPT3). Increasing the number of in-context examples has only been explored recently by a few works [ 7 C 9 ]. Both Li et al.  [7]  and Agarwal et al.  [8]  explore scaling in-context learning to more than 1,000 demonstrating examples and find performance improvements across multiple tasks. However, their experiments are limited to text-only benchmarks and do not compare performance across different models.  

Multimodal ICL.  Due to the recent emergence of LMMs, research on multimodal ICL is still nascent. One prior work developed a new model to leverage complex prompts composed of multimodal inputs in order to allow models to compare images [ 10 ], while other recent works explored the general iz ability of GPT-4V and Gemini to multimodal out-domain and out-of-distribution tasks, and found that ICL leads to performance benefits for both models across many tasks [ 6 ,  5 ]. However, none of these works have leveraged the new largely expanded context windows to investigate the effects of increasing the number of demonstrating examples.  

Batch Querying.  Multiple prior works have explored batching queries (also commonly referred to as batch prompting) for more efficient and cheaper inference. Batch prompting was first introduced in Cheng et al.  [11] , leading to comparable or better performance than single prompting, while achieving substantially reduced inference token cost and latency. Lin et al.  [12]  observe performance degradation with batched prompts in longer contexts, and propose a variety of techniques to mitigate the performance loss. More recently, additional variations of batch prompting have been proposed, including grouping similar questions together [ 13 ], batching prompts of different tasks [ 14 ], and concatenating multiple images into a single image collage [ 15 ]. We again note that batch prompting with high numbers of demonstrating examples and high numbers of queries has only become feasible due to larger context windows of recent models.  

# 3 Methods  

We conduct several experiments to test the effect of increasing the number of demonstrating examples on the performance of two state-of-the-art multimodal foundation models: GPT-4o and Gemini 1.5 Pro (Section 3.1). We benchmark their performance using standard performance metrics as well as an ICL data efficiency metric (Section 3.3) on 10 datasets spanning several vision domains and image classification tasks (Section 3.2). We conduct ablation studies to test the impact of batching queries on model performance and explain the substantial improvement in zero-shot settings (Section 4.2). We refer to the many-shot in-context learning framework as many-shot ICL. Figure 1 provides an illustrative summary of many-shot ICL and batched many-shot ICL compared to zero-shot and few-shot ICL.  

# 3.1 Models  

We use three state-of-the-art multimodal foundation models with public API access, namely GPT-4o, GPT4(V)-Turbo [ 4 ], and Gemini 1.5 Pro [ 16 ]. Because GPT-4o performs substantially better than GPT4(V)-Turbo, we focus on the results of GPT-4o and Gemini 1.5 Pro in the main text, and include GPT4(V)-Turbo results in the Appendix. We do not utilize Claude3-Opus in our experiments, as it only accepts up to 20 images in one request at the time of writing. The specific endpoint for for GPT-4o is “gpt-4o-2024-05-13”, for GPT-4(V)-Turbo is “gpt-4-turbo-2024-04-09”, and for Gemini  $1.5\;\mathrm{Pro}$   is “gemini-1.5-pro-preview-  $.0409^{\ast}$  . We use the API service provided by OpenAI for GPT-4o and GPT-4(V)-Turbo, and the API service provided by Google Cloud on Vertex AI for Gemini 1.5 Pro. We set the temperature to zero for all models and a random seed for GPT-4(V)-Turbo and GPT-4o to obtain more deterministic responses. To prevent models from abstaining (which happens rarely), we rerun the query until an answer is provided.  

# 3.2 Datasets  

We benchmark the model performance on 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification). We choose to focus on image classification tasks as other tasks such as region captioning would require substantially more tokens thereby limiting the total number of demonstrating examples, and most LMMs are not yet capable of accurately producing localizations required for other tasks like bounding boxes and segmentation masks [ 17 ,  18 ]. Table 1 provides a summary of the datasets used in this study.  

For all datasets, we construct a set of demonstration (demo) examples from the original training and validation splits used for in-context learning and a test set from the original test split (if one exists) to evaluate the performance of the models. We randomly sample the demo and test sets from the original dataset without replacement. For the multi-class and fine-grained classification datasets, we perform a class-stratified sampling, ensuring an equal number of examples per class in both the demo and test sets. For the multi-label classification dataset (CheXpert), we sample an equal number of positive and negative samples per class in both the demo and test sets. We note that, since the task is multi-label, this sampling procedure does not result in an exactly equal number of examples per class. The per-dataset sizes of the full demo and test sets are shown in Table 1, and we increase the number of demonstration examples up to the numbers shown in the table while ensuring class balance for the scaling experiments.  

# 3.3 Evaluation Metrics  

We use standard metrics to evaluate model performance on each dataset. Specifically, we measure performance using accuracy for all multi-class classification datasets as they are sampled to have a balanced class distribution. For multi-label classification on CheXpert, we use the macro-averaged F1 metric. In the rare case of parsing errors, we consider the response as incorrect. To estimate the variability around the evaluation metrics, we compute standard deviation using bootstrapping with 1,000 bootstrap replicates.  

In addition to standard performance metrics, we measure the data efficiency of each model. Specifi- cally, we compute a linear regression between  $\log_{10}(N+1)$   (with    $N$   the number of examples) and model performance, enforcing that the line passes through the zero-shot performance point. This value approximates the amount of performance improvement from zero-shot expected from including an order of magnitude more demonstrating examples.  

Table 1:  Summary of benchmark datasets.  We use 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification).  

![](images/7205245d4d93aea120e20581767010eac80e677a2b8fa68813b14b64b9b19c77.jpg)  

![Table 2:  Many-shot ICL performance and efficiency comparison.  We report the performance under a zero-shot regime and performance at the optimal demo set size as well as the many-shot ICL data efficiency of GPT-4o and Gemini  $1.5~\mathrm{Pro}$  . We measure performance using accuracy on all datasets except CheXpert, for which we use macro-average F1. We bold the highest ICL data efficiency between the two models on each dataset. ](images/5747989fbf490aa5015d41f600c67f18638cc9a72f5f650cee24797078ec72b4.jpg)  

# 4 Results  

We present many-shot ICL performance using batched queries in Section 4.1, investigate the impact of batching queries on performance in Section 4.2, and provide an analysis on cost and latency in Section 4.3. Results using GPT4(V)-Turbo are in Appendix C.  

# 4.1 Increasing number of demonstrating examples  

Main Results.  Gemini 1.5 Pro exhibits consistent and substantial improvements as the number of demonstrating examples increases across all datasets except for DrugOOD Assay (Figure 2). Gemini  $1.5\;\mathrm{Pro}$   shows particularly large improvements from many-shot ICL on HAM10000 (  ${\it+23\%}$   accuracy compared to zero-shot,  $+16\%$   compared to 7 examples), FIVES   ${\it\Omega}+29\%$   compared to zero-shot,  $+27\%$  compared to 20 examples), and EuroSAT   $(+38\%$   compared to zero-shot,  $+31\%$   compared to 10 examples). Notably, for 5 out of the 10 datasets (FIVES, UCMerced, EuroSAT, Oxford Pets, and DTD), Gemini 1.5 Pro performance continues to improve up to the highest number of demonstrating examples considered   $(\mathord{\sim}1,\!000$   examples). On the other 5 datasets, the optimal performance occurs prior to the highest number of demo examples, with the maximum number of demo examples leading to similar or slightly worse performance than the optimal demo set size. On the other hand, Gemini 1.5 Pro performance on DrugOOD Assay does not substantially benefit from many-shot ICL, with high variance in performance across demo sizes and the peak performance at 40 demo examples.  

Similarly, GPT-4o shows substantial performance improvements on all datasets except FIVES and DrugOOD Assay using many-shot ICL, but the improvement is not consistent. For many datasets, performance drops sharply at first and then improves significantly as the number of demonstrating examples increases further, resulting in V-shaped scaling curves (Figure 2). We also note that we were unable to increase the number of demo examples to the same level as considered for Gemini 1.5 Pro because GPT-4o has a shorter context window and is more prone to timeout errors with longer inputs. GPT-4o performance on DrugOOD Assay shows high variance, similar to Gemini  $1.5\;\mathrm{Pro}$  , with the peak performance observed at 50 demo examples.  

Sensitivity to prompt selection.  We also explore a different set of prompts to test the robustness of many-shot ICL to differences in prompt wording on two datasets. While there is a small deviation in performance between different prompts, the overall log-linear improvement trend is consistent across the prompts. Details can be found in Appendix B.  

ICL data efficiency.  We find Gemini   $1.5\;\mathrm{Pro}$   demonstrates higher ICL data efficiency than GPT- 4o across all datasets except TerraIncognita and DTD (Table 2). Gemini 1.5 Pro ICL efficiency is especially high on EuroSAT, with  $20.61\%$   improvement in accuracy for every   $10\mathbf{x}$   more demo examples, and lowest on DrugOOD Assay (2.03), Camelyon17 (3.00), and TerraIncognita (3.50). GPT-4o ICL data efficiency is especially high on TerraIncognita   $(20.50\%)$   and EuroSat (19.40).  

![](images/bff391610e9c220cf73f2868432fb8d09d38554b2955e4c0cc37c21cee960108.jpg)  
Figure 3:  Gemini 1.5 Pro performance under many-shot and zero-shot ICL when varying the amount of queries included in every request.  We show performance per batch size with the optimal number of demo examples (many-shot) and no demo examples (zero-shot). The  $x$  -axis is in log scale. Under the many-shot regime, batching queries leads to no substantial drop in performance compared to individual queries when we choose a suitable batch size. For zero-shot, including only one query is suboptimal for many datasets.  

Gemini 1.5 Pro has a positive efficiency on all datasets and GPT-4o has a positive data efficiency on 9 of the 10 datasets (excluding Oxford Pets). Importantly, both models benefit substantially from many-shot ICL at the optimal demo set size, with an average improvement of  $+17\%$   for both Gemini 1.5 Pro and GPT-4o.  

# 4.2 Impact of batching queries  

As including a large set of demo examples in the prompt leads to much longer sequence lengths and therefore higher inference time and cost, we consider batching queries in a single prompt to reduce per-query cost, and examine the impact of different batch sizes on model performance. Due to its superior performance and free preview access, we use Gemini   $1.5\;\mathrm{Pro}$   for these experiments.  

Main Results.  We find minimal performance degradations, and sometimes performance improve- ments, as we increase the number of queries included in each batch across under both zero-shot and many-shot (at the optimal demo set size) regimes (Figure 3). Notably, using a single query each time with many-shot ICL is suboptimal across many of the datasets. We find that the optimal batch size is among the three largest sizes on every dataset except CheXpert and EuroSAT, which both see optimal performance with a single query at a time.  

We additionally observe that including a single query at a time is suboptimal on most datasets in the zero-shot regime. Surprisingly, performance with the highest batch size is substantially higher across three datasets under the zero-shot regime, with a consistent performance improvement as the batch size is increased on both UCMerced and Terraincognita.  

Zero-shot performance improvements from batching queries.  We conduct several additional experiments to investigate why batch querying can lead to large performance improvements under the zero-shot regime on TerraIncognita and UCMerced. We hypothesize that this improvement may be due to three potential benefits from ICL: (1) domain calibration, where the model benefits from seeing more images in the domain in order to adapt to it, (2) class calibration, where seeing images of different classes enables the model to better calibrate its outputs, and (3) self-ICL (shown to be effective in prior work [ 29 ]), where the model can learn from self-generated demonstrations due to autoregressive decoding. We design experiments to isolate the potential benefits from each of these types of ICL between asking a single query to batching 50 queries together.  

First, to measure potential improvement from domain calibration, we include 49 images from the same class in the prompt without including any label. We find a  $3.0\%$   improvement on TerraIncognita  

![](images/d448fc079ac55d9535c79b6dcb9cd01f084aad34cad1d391283da4d8103570e9.jpg)  
Figure 4:  Ablation study to investigate why batching queries leads to performance improve- ments when using Gemini  $\mathbf{1.5\,Pr0}$   in a zero-shot setting.  The first bar shows performance when including a single query, the second adds 49 unlabeled images from a single class, the third adds 49 unlabeled images in total from all classes, the fourth adds model responses to include self-generated demonstrations, and the last includes 50 queries in one request.  

and  $2.6\%$   degradation on UCMerced, suggesting domain calibration is helpful for the former but not the latter. Second, to capture performance gains from class calibration, we include a random sample of 49 images in the prompt, again without including the label. We see a further  $3.5\%$   improvement on TerraIncognita (  $6.5\%$   improvement from a single query) and a  $4.5\%$   improvement from a single query on UCMerced, suggesting including the context of class-balanced images is helpful even without labels. Third, to capture additional performance improvements from the self-generated labels, we obtain predicted labels from the zero-shot model using a single query for each of the 49 randomly sampled images and add them to the prompt. We observe further performance increase on both datasets, with  $5.5\%$   on TerraIncognita and  $2.7\%$   on UCMerced. The final total accuracy is similar to asking the 50 questions each round, which suggests these three components mostly explain the reason for improved zero-shot performance under a larger query batch size.  

# 4.3 Cost and latency analysis  

Many-shot ICL incurs zero additional training cost, but per-query inference can be costly and slow due to long input contexts. To quantitatively measure this, we compute the latency and cost associated with the zero-shot and many-shot requests with and without batching when using Gemini   $1.5\;\mathrm{Pro}$   on HAM10000 and TerraIncognita. We calculate the costs using the Gemini 1.5 Pro preview pricing (  $\S7$   per 1 million input tokens and   $\S21$   per 1 million output tokens). For fair comparison and to minimize data transfer artifacts, all requests are sent to the same location where the VM instance is held (“us-central1”). We run the query three times under each setting and report the average.  

In the zero-shot regime, we see substantial per-example latency reductions due to query batching, close to a  $10\mathbf{x}$   reduction on HAM10000 and  $2\mathbf{X}$   on TerraIncognita (Table 3). The per-example cost is similar between the two as there is no additional context needed for including demonstrating examples. In the many-shot regime, we observe substantial reductions in both per-example latency and cost on both datasets. Specifically, for HAM10000, we find a near  $35\mathrm{x}$   reduction in latency and  $10\mathbf{x}$   reduction in cost, and   $20\mathbf{x}$   reduction in latency and  $45\mathbf{x}$   reduction in cost for TerraIncognita.  

# 5 Discussion  

In this study, we evaluate many-shot ICL of state-of-the-art multimodal foundation models across 10 datasets and find consistent performance improvements across most of the datasets. Batching queries with many-shot ICL further exhibits substantially reduced per-example latency and inference costs without compromising performance.  

Our findings suggest that these multimodal foundation models have the capability of performing ICL with large numbers of demonstrating examples, which may have significant implications on their practical use. For example, it was previously impossible to adapt these large, private models to  

![Table 3:  Inference latency and cost using Gemini 1.5 Pro with and without query batching.  We use 50 queries per batch. In the zero-shot setting, we can achieve lower per-example latency with batching, but the per-example cost remains identical. In the many-shot setting, the per-example cost and per-example latency both drop substantially with query batching. ](images/08cad9838361c7c5255cbdae6c004feabab7f6d3d3cb985b2b076061aef4f1a1.jpg)  

new tasks and domains, but many-shot ICL would enable users to leverage demonstrating examples to adapt the models. One significant advantage of many-shot ICL is its ability to get quick results even on the same day of model release, and that’s why we can finish our evaluation using GPT-4o within days. Furthermore, fine-tuning open-source models is the standard practice when practitioners have access to moderately sized datasets, but many-shot ICL may remove the need for fine-tuning, making it much easier to develop customized approaches. We note that it remains to be seen how traditional fine-tuning of these models compares to many-shot ICL with foundation models in terms of absolute performance and data efficiency, so future work should explore this. In addition, it is important to study general issues which plague those foundation models, such as hallucinations and biases, under the context of many-shot ICL and batching queries. For example, it would be interesting to explore if carefully curated and large sets of demonstrating examples can reduce biases across different sub-groups. We leave this to future work.  

Our study has limitations. First, we only explore performance under many-shot ICL on image classification tasks and with private foundation models. We believe these are the most practically relevant and common multimodal settings, but it is worthwhile for future work to explore potential benefits from many-shot ICL on other tasks and with upcoming open-source multimodal foundation models like LLaMA-3 [ 30 ]. Second, even after recent developments to increase context size, the size prohibits many-shot ICL from being used on datasets with a large number (several hundred or more) of classes. We anticipate that context window sizes will continue to increase in size over time which will mitigate this issue. Third, the datasets which were used to train these private models have not been disclosed, so it is difficult to tell whether the models have been trained on the datasets we selected. We argue that zero-shot performance across the datasets is far from perfect which provides evidence that the datasets have not been used for training, but we cannot determine that with certainty.  

# 6 Conclusion  

In summary, we show that multimodal foundation models are capable of many-shot ICL. We believe that these results pave a promising path forward to improve the adaptability and accessibility of large multimodal foundation models.  

# References  

[1]  Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.  

[2]  Archit Parnami and Minwoo Lee. Learning from few examples: A summary of approaches to few-shot learning.  arXiv preprint arXiv:2203.04291 , 2022.  

[3]  Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples: A survey on few-shot learning.  ACM computing surveys (csur) , 53(3):1C34, 2020.

 [4]  Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.  arXiv preprint arXiv:2303.08774 , 2023.

 [5]  Zhongyi Han, Guanglin Zhou, Rundong He, Jindong Wang, Xing Xie, Tailin Wu, Yilong Yin, Salman Khan, Lina Yao, Tongliang Liu, et al. How well does gpt-4v (ision) adapt to distribution shifts? a preliminary investigation.  arXiv preprint arXiv:2312.07424 , 2023.

 [6]  Xingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia Hai, Renzhe Xu, Yuqing Yang, Shikai Guan, Jiazheng Xu, and Peng Cui. On the out-of-distribution generalization of multimodal large language models.  arXiv preprint arXiv:2402.06599 , 2024.

 [7]  Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, Zhiyong Wu, and Lingpeng Kong. In-context learning with many demonstration examples.  arXiv preprint arXiv:2302.04931 , 2023.

 [8]  Rishabh Agarwal, Avi Singh, Lei M Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John D Co-Reyes, Eric Chu, et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018 , 2024.

 [9]  Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R Gormley, and Graham Neubig. In-context learning with long-context models: An in-depth exploration.  arXiv preprint arXiv:2405.00200 , 2024.

 [10]  Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with multi-modal in-context learning.  arXiv preprint arXiv:2309.07915 , 2023.

 [11]  Zhoujun Cheng, Jungo Kasai, and Tao Yu. Batch prompting: Efficient inference with large language model apis.  arXiv preprint arXiv:2301.08721 , 2023.

 [12]  Jianzhe Lin, Maurice Diesendruck, Liang Du, and Robin Abraham. Batchprompt: Accomplish more with less.  arXiv preprint arXiv:2309.00384 , 2023.

 [13]  Jiayi Liu, Tinghan Yang, and Jennifer Neville. Cliqueparcel: An approach for batching llm prompts that jointly optimizes efficiency and faithfulness.  arXiv preprint arXiv:2402.14833 , 2024.

 [14]  Guijin Son, Sangwon Baek, Sangdae Nam, Ilgyun Jeong, and Seungone Kim. Multi-task inference: Can large language models follow multiple instructions at once?  arXiv preprint arXiv:2402.11597 , 2024.

 [15]  Siyu Xu, Yunke Wang, Daochang Liu, and Chang Xu. Collage prompting: Budget-friendly visual recognition with gpt-4v.  arXiv preprint arXiv:2403.11468 , 2024.

 [16]  Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean- baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.  arXiv preprint arXiv:2403.05530 , 2024.

 [17]  Yixuan Wu, Yizhou Wang, Shixiang Tang, Wenhao Wu, Tong He, Wanli Ouyang, Jian Wu, and Philip Torr. Dettoolchain: A new prompting paradigm to unleash detection ability of mllm. arXiv preprint arXiv:2403.12488 , 2024.

 [18]  Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection with multimodal large language models.  arXiv preprint arXiv:2305.18279 , 2023.

 [19]  Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions.  Scientific data , 5(1): 1C9, 2018.  

[20]  Kai Jin, Xingru Huang, Jingxing Zhou, Yunxiang Li, Yan Yan, Yibao Sun, Qianni Zhang, Yaqi Wang, and Juan Ye. Fives: A fundus image dataset for artificial intelligence based vessel segmentation.  Scientific Data , 9(1):475, 2022.

 [21]  Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In  Proceedings of the AAAI conference on artificial intelligence , volume 33, pages 590C597, 2019.

 [22]  Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge.  IEEE transactions on medical imaging , 38(2):550C560, 2018.

 [23]  Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In  Proceedings of the European conference on computer vision (ECCV) , pages 456C473, 2018.

 [24]  Yi Yang and Shawn Newsam. Bag-of-visual-words and spatial extensions for land-use clas- sification. In  Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems , pages 270C279, 2010.

 [25]  Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.  IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing , 12(7):2217C2226, 2019.

 [26]  Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In  2012 IEEE conference on computer vision and pattern recognition , pages 3498C3505. IEEE, 2012.

 [27]  Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 3606C3613, 2014.

 [28]  Yuanfeng Ji, Lu Zhang, Jiaxiang Wu, Bingzhe Wu, Long-Kai Huang, Tingyang Xu, Yu Rong, Lanqing Li, Jie Ren, Ding Xue, et al. Drugood: Out-of-distribution (ood) dataset curator and benchmark for ai-aided drug discoveryCa focus on affinity prediction problems with noise annotations.  arXiv preprint arXiv:2201.09637 , 2022.

 [29]  Wei-Lin Chen, Cheng-Kuang Wu, and Hsin-Hsi Chen. Self-icl: Zero-shot in-context learning with self-generated demonstrations.  arXiv preprint arXiv:2305.15035 , 2023.

 [30]  Introducing meta llama 3: The most capable openly available llm to date. URL  https: //ai.meta.com/blog/meta-llama-3/ .  

# A Prompts used for ICL experiments  

# A.1 Prompt used for image classification experiments  

prompt   $=$   "" for demo in demo_examples: prompt   $+=$   f"""  $<<\tt I M G>>G$  iven the image above, answer the following question- using the specified format. Question: What is in the image above? Choices: {str(class_desp)} Answer Choice: {demo.answer} """  

prompt   $+=$   f"""<<IMG  $>>$  Given the image above, answer the following question- using the specified format. Question: What is in the image above? Choices: {str(class_desp)}  

Please respond with the following format: ---BEGIN FORMAT TEMPLATE---Answer Choice: [Your Answer Choice Here] Confidence Score: [Your Numerical Prediction Confidence Score Here From 0 To 1] ---END FORMAT TEMPLATE---  

Do not deviate from the above format. Repeat the format template for the answer."""  

# A.2 Prompts used for image classification experiments with batching  

prompt   $=$   "" for demo in demo_examples: prompt   $+=$   f"""<<IMG>>Given the image above, answer the following question- using the specified format. Question: What is in the image above? Choices: {str(class_desp)} Answer Choice: {demo[1]}  

for idx, i in enumerate(test_df.iloc[start_idx:end_idx].itertuples()): prompt   $+=$   f"""<<IMG>>Given the image above, answer the following question- using the specified format. Question {qn_idx}: What is in the image above? Choices {qn_idx}: {str(class_desp)}  

"""  

for i in range(start_idx, end_idx): qn_idx   $=$   i-start_idx  $+1$  prompt   $+=$   f""" Please respond with the following format for each question: ---BEGIN FORMAT TEMPLATE FOR QUESTION {qn_idx}---Answer Choice {qn_idx}: [Your Answer Choice Here for Question {qn_idx}] Confidence Score {qn_idx}: [Your Numerical Prediction Confidence Score Here- From 0 To 1 for Question {qn_idx}] ---END FORMAT TEMPLATE FOR QUESTION {qn_idx}---  

Do not deviate from the above format. Repeat the format template for the answer."""  

# A.3 Prompts used for batching ablation experiments  

# A.3.1 Prefixing images  

prompt   $=$   "" for demo in prefix image paths: prompt   $+=$   f"""<<IMG>> """ prompt   $+=$   "Above are some images from the same dataset. " qns_idx  $\begin{array}{r l}{\mathbf{\Sigma}=}&{{}\left[\begin{array}{l l l}\end{array}\right]}\end{array}$  for idx, i in enumerate(test_df.iloc[start_idx:end_idx].itertuples()): qn_idx   $=$   idx  $^{+1}$  prompt   $+=$   f"""<<IMG>> Given the image above, answer the following question- using the specified format. Question {qn_idx}: What is in the image above? Choices {qn_idx}: {str(class_desp)} """ for i in range(start_idx, end_idx): qn_idx   $=$   i-start_idx  $+1$  prompt   $+=$   f""" Please respond with the following format for each question: ---BEGIN FORMAT TEMPLATE FOR QUESTION {qn_idx}---Answer Choice {qn_idx}: [Your Answer Choice Here for Question {qn_idx}] Confidence Score {qn_idx}: [Your Numerical Prediction Confidence Score Here- From 0 To 1 for Question {qn_idx}] ---END FORMAT TEMPLATE FOR QUESTION {qn_idx}---  

Do not deviate from the above format. Repeat the format template for the answer."""  

# B Prompt selection  

We utilize a different set of prompts to test the robustness of ManyICL to differences in prompt wording. We randomly sample two datasets (HAM10000 and EuroSAT) for this experiment due to budget limit.  

# B.1 Prompts used for prompt selection experiments  

Note that only the question section is shown here, and prompt 1 is used for all other image classifica- tion experiments.  

# B.1.1 Prompt 1  

<<IMG>>Given the image above, answer the following question using the specified format. Question {qn_idx}: What is in the image above? Choices {qn_idx}: {str(class_desp)}  

# B.1.2 Prompt 2  

<<IMG>>Given the image above, answer the following question using the specified format. Question {qn_idx}: Which class does this image belong to? Choices {qn_idx}: {str(class_desp)}  

# B.1.3 Prompt 3  

Question {qn_idx}: <<IMG>>Classify the image above, choose from {str(class_desp)}  

![](images/99b88c369400230a2a9184c28d29b1a0a33e58585e37cb97682df3efb971c013.jpg)  
Figure 5:  Sensitivity analysis of many-shot ICL.  These plots show the change in task performance on two datasets as the number of demonstrating examples increases, using three different prompts. For all experiments on sensitivity analysis, the Gemini 1.5 Pro model is used. The  $x$  -axis is in the logarithmic scale, representing the number of demonstrating examples plus one. The log-linear improvement until the optimal performance is consistent across all prompts selected.  

# B.2 Prompt selection results  

Figure 5 shows the sensitivity of performance to prompt selection on two datasets with three prompts. While there exists a small deviation in performance, but the overall log-linear improvement trend is consistent.  

# C GPT4(V)-Turbo performance under many-shot ICL  

GPT4(V)-Turbo shows mixed results for many-shot ICL, with substantial performance improvements on HAM1000, UCMerced, EuroSAT, and DTD, but minimal improvements or no improvement across the other six datasets (Figure 6). However, we note that we were unable to increase the number of demo examples to the same level as Gemini 1.5 Pro because GPT4(V)-Turbo has a shorter context window and is more prone to timeout errors when scaling. Additionally, GPT4(V)-Turbo seems to generally underperform Gemini   $1.5\;\mathrm{Pro}$   across the datasets excluding FIVES and EuroSAT for which it seems to mostly match the Gemini 1.5 Pro performance. GPT4(V)-Turbo performance on DrugOOD Assay shows high variance, resembling that of Gemini 1.5 Pro with the peak performance at 40 demo examples.  

# D Performance of many-shot ICL on medical QA tasks  

# D.1 Prompt used for medical QA experiments (MedQA, MedMCQA)  

prompt   $=$   "You are an expert in answering medical exam questions. " for demo in demo_examples: prompt   $+=$   f"""Question: {demo.question} Choices: {demo.options} Answer: {demo.answer}  

prompt   $+=$   f"""Question: {actual.question} Choices: {actual.options}  

![](images/7fbb1117324c771ddde9abff02886943aa7c4fe74670ff3b2eecb97784e88c53.jpg)  
Figure 6:  GPT4(V)-Turbo and GPT-4o performance from zero-shot to many-shot ICL.  X-axis is in log scale.  

![](images/7fa5e3c838b281a806b1b13a640b9052e8547e81867ca302096902f370c707f6.jpg)  
Figure 7: Many-shot ICL performances of medical QA tasks.  

Do not deviate from the above format. Repeat the format template for the answer."""  

# D.2 Results  

Figure 7 shows the results on medical QA tasks.  

# Acknowledgments and Disclosure of Funding  

We thank Dr. Jeff Dean, Yuhui Zhang, Dr. Mutallip Anwar, Kefan Dong, Rishi Bommasani, Ravi B. Sojitra, Chen Shani and Annie Chen for their feedback on the ideas and manuscript. Yixing Jiang is supported by National Science Scholarship (PhD). This work is also supported by Google cloud credit. Dr. Jonathan Chen has received research funding support in part by NIH/National Institute of Allergy and Infectious Diseases (1R01AI17812101), NIH/National Institute on Drug Abuse Clinical Trials Network (UG1DA015815 - CTN-0136), Gordon and Betty Moore Foundation (Grant #12409), Stanford Artificial Intelligence in Medicine and Imaging - Human-Centered Artificial Intelligence (AIMI-HAI) Partnership Grant, Google, Inc. Research collaboration Co-I to leverage EHR data to predict a range of clinical outcomes, American Heart Association - Strategically Focused Research Network - Diversity in Clinical Trials and NIH-NCATS-CTSA grant (UL1TR003142) for common research resources.  
2024-08-19 16:22:15,629 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1.md: 文件路径无效: # Many-Shot In-Context Learning in Multimodal Foundation Models  

# Yixing Jiang \u2217 Jeremy Irvin \u2217 Ji Hun Wang Muhammad Ahmed Chaudhry Jonathan H. Chen Andrew Y. Ng  

Stanford University {jiang6,jirvin16,jihunwang,ang}@cs.stanford.edu {mahmedch,jonc101}@stanford.edu  

# Abstract  

Large language models are well-known to be effective at few-shot in-context learning (ICL). Recent advancements in multimodal foundation models have en- abled unprecedentedly long context windows, presenting an opportunity to explore their capability to perform ICL with many more demonstrating examples. In this work, we evaluate the performance of multimodal foundation models scaling from few-shot to many-shot ICL. We benchmark GPT-4o and Gemini   $1.5\ \mathrm{Pr}\bar{\mathrm{o}}$  across 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification). We observe that many-shot ICL, including up to al- most 2,000 multimodal demonstrating examples, leads to substantial improvements compared to few-shot   $\langle<\!100$   examples) ICL across all of the datasets. Further, Gemini   $1.5~\mathrm{Pro}$   performance continues to improve log-linearly up to the max- imum number of tested examples on many datasets. Given the high inference costs associated with the long prompts required for many-shot ICL, we also ex- plore the impact of batching multiple queries in a single API call. We show that batching up to 50 queries can lead to performance improvements under zero-shot and manyCshot ICL, with substantial gains in the zero-shot setting on multiple datasets, while drastically reducing per-query cost and latency. Finally, we measure ICL data efficiency of the models, or the rate at which the models learn from more demonstrating examples. We find that while GPT-4o and Gemini   $1.5~\mathrm{Pro}$  achieve similar zero-shot performance across the datasets, Gemini 1.5 Pro exhibits higher ICL data efficiency than GPT-4o on most datasets. Our results suggest that many-shot ICL could enable users to efficiently adapt multimodal foundation models to new applications and domains. Our codebase is publicly available at https://github.com/stanfordmlgroup/ManyICL .  

# 1 Introduction  

Large language models (LLMs) have been shown to substantially benefit from the inclusion of a few demonstrating examples ( shots ) in the LLM context before the test query [ 1 C 3 ]. This phenomenon, commonly referred to as in-context learning (ICL), enables LLMs to learn from few shots without any updates to model parameters, and therefore improves specialization to new tasks without any further model training. More recently, large multimodal models (LMMs) have also demonstrated the capability of learning from in-context examples [4C6]. Han et al. [5] and Zhang et al. [6] both show that few-shot multimodal ICL specifically helps to improve LMM performance on out-domain or out-of-distribution tasks.  

![](images/c2295c09026975f63765663f5663420aa41ecabe03fed3062728054bfde7fa4a.jpg)  
Figure 1:  Many-shot multimodal in-context learning compared to zero-shot and few-shot multimodal ICL.  In zero-shot and few-shot settings, respectively, no demonstrating examples or only a small number of demonstrating examples are provided in the context before the test query. In a many-shot ICL setting, we include a large number of demonstrating examples in the prompt, whereas in batched many-shot ICL, we perform multiple queries at once using query references.  

While few-shot ICL has enabled promising performance improvements for both LLMs and LMMs, limited model context windows have constrained research on the impact of increasing the number of demonstrating examples on performance. This is especially true for LMMs as most use a large number of visual tokens to represent images. However, due to recent advancements enabling substantially longer context windows C for example, 128,000 tokens for GPT-4o and up to one million tokens for Gemini 1.5 Pro C it is now possible to explore the effect of drastically increasing the number of demonstrating examples.  

To investigate the capability of state-of-the-art multimodal foundation models to perform many-shot ICL, we conduct a large suite of experiments benchmarking model performance on 10 datasets spanning several domains and image classification tasks after scaling up the number of demonstrating examples by multiple orders of magnitude. Specifically, our contributions are as follows:  

1.  We show that providing multimodal foundation models with many demonstrating examples leads to substantial performance improvements compared to providing only a few demon- strating examples. We observe that the performance of Gemini 1.5 Pro generally improves log-linearly as the number of demonstrating examples increases, whereas GPT-4o exhibits less stable improvements as the number of in-context examples increases. 2.  We measure the data efficiency of the models under ICL as the number of demonstrating examples is increased, and find that Gemini  $1.5\;\mathrm{Pro}$   exhibits higher ICL data efficiency than GPT-4o on most datasets. 3.  We demonstrate that batching multiple queries into a single request can achieve similar or better performance than single query requests in a many-shot setting, while enabling substantially lower per-example latency and much cheaper per-example inference cost. 4.  We find that batching multiple questions can lead to substantial performance improvements in a zero-shot setting. We design experiments to explain this phenomenon, and find that the improvements are due to a combination of domain calibration, class calibration, and self-generated demonstrating examples due to autoregressive decoding.  

![](images/37c467c4191e2fac2d692a0fab388edb7e621823aa9ac0d60e897b50ce2cfee8.jpg)  

# 2 Related Work  

Scaling ICL.  The seminal work of Brown et al.  [1]  discovered performance improvements for LLMs from increasing the number of in-context examples, but the tested number of demonstrating examples was low (10 to 100), likely due to the restrictive context size (2048 tokens for GPT3). Increasing the number of in-context examples has only been explored recently by a few works [ 7 C 9 ]. Both Li et al.  [7]  and Agarwal et al.  [8]  explore scaling in-context learning to more than 1,000 demonstrating examples and find performance improvements across multiple tasks. However, their experiments are limited to text-only benchmarks and do not compare performance across different models.  

Multimodal ICL.  Due to the recent emergence of LMMs, research on multimodal ICL is still nascent. One prior work developed a new model to leverage complex prompts composed of multimodal inputs in order to allow models to compare images [ 10 ], while other recent works explored the general iz ability of GPT-4V and Gemini to multimodal out-domain and out-of-distribution tasks, and found that ICL leads to performance benefits for both models across many tasks [ 6 ,  5 ]. However, none of these works have leveraged the new largely expanded context windows to investigate the effects of increasing the number of demonstrating examples.  

Batch Querying.  Multiple prior works have explored batching queries (also commonly referred to as batch prompting) for more efficient and cheaper inference. Batch prompting was first introduced in Cheng et al.  [11] , leading to comparable or better performance than single prompting, while achieving substantially reduced inference token cost and latency. Lin et al.  [12]  observe performance degradation with batched prompts in longer contexts, and propose a variety of techniques to mitigate the performance loss. More recently, additional variations of batch prompting have been proposed, including grouping similar questions together [ 13 ], batching prompts of different tasks [ 14 ], and concatenating multiple images into a single image collage [ 15 ]. We again note that batch prompting with high numbers of demonstrating examples and high numbers of queries has only become feasible due to larger context windows of recent models.  

# 3 Methods  

We conduct several experiments to test the effect of increasing the number of demonstrating examples on the performance of two state-of-the-art multimodal foundation models: GPT-4o and Gemini 1.5 Pro (Section 3.1). We benchmark their performance using standard performance metrics as well as an ICL data efficiency metric (Section 3.3) on 10 datasets spanning several vision domains and image classification tasks (Section 3.2). We conduct ablation studies to test the impact of batching queries on model performance and explain the substantial improvement in zero-shot settings (Section 4.2). We refer to the many-shot in-context learning framework as many-shot ICL. Figure 1 provides an illustrative summary of many-shot ICL and batched many-shot ICL compared to zero-shot and few-shot ICL.  

# 3.1 Models  

We use three state-of-the-art multimodal foundation models with public API access, namely GPT-4o, GPT4(V)-Turbo [ 4 ], and Gemini 1.5 Pro [ 16 ]. Because GPT-4o performs substantially better than GPT4(V)-Turbo, we focus on the results of GPT-4o and Gemini 1.5 Pro in the main text, and include GPT4(V)-Turbo results in the Appendix. We do not utilize Claude3-Opus in our experiments, as it only accepts up to 20 images in one request at the time of writing. The specific endpoint for for GPT-4o is “gpt-4o-2024-05-13”, for GPT-4(V)-Turbo is “gpt-4-turbo-2024-04-09”, and for Gemini  $1.5\;\mathrm{Pro}$   is “gemini-1.5-pro-preview-  $.0409^{\ast}$  . We use the API service provided by OpenAI for GPT-4o and GPT-4(V)-Turbo, and the API service provided by Google Cloud on Vertex AI for Gemini 1.5 Pro. We set the temperature to zero for all models and a random seed for GPT-4(V)-Turbo and GPT-4o to obtain more deterministic responses. To prevent models from abstaining (which happens rarely), we rerun the query until an answer is provided.  

# 3.2 Datasets  

We benchmark the model performance on 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification). We choose to focus on image classification tasks as other tasks such as region captioning would require substantially more tokens thereby limiting the total number of demonstrating examples, and most LMMs are not yet capable of accurately producing localizations required for other tasks like bounding boxes and segmentation masks [ 17 ,  18 ]. Table 1 provides a summary of the datasets used in this study.  

For all datasets, we construct a set of demonstration (demo) examples from the original training and validation splits used for in-context learning and a test set from the original test split (if one exists) to evaluate the performance of the models. We randomly sample the demo and test sets from the original dataset without replacement. For the multi-class and fine-grained classification datasets, we perform a class-stratified sampling, ensuring an equal number of examples per class in both the demo and test sets. For the multi-label classification dataset (CheXpert), we sample an equal number of positive and negative samples per class in both the demo and test sets. We note that, since the task is multi-label, this sampling procedure does not result in an exactly equal number of examples per class. The per-dataset sizes of the full demo and test sets are shown in Table 1, and we increase the number of demonstration examples up to the numbers shown in the table while ensuring class balance for the scaling experiments.  

# 3.3 Evaluation Metrics  

We use standard metrics to evaluate model performance on each dataset. Specifically, we measure performance using accuracy for all multi-class classification datasets as they are sampled to have a balanced class distribution. For multi-label classification on CheXpert, we use the macro-averaged F1 metric. In the rare case of parsing errors, we consider the response as incorrect. To estimate the variability around the evaluation metrics, we compute standard deviation using bootstrapping with 1,000 bootstrap replicates.  

In addition to standard performance metrics, we measure the data efficiency of each model. Specifi- cally, we compute a linear regression between  $\log_{10}(N+1)$   (with    $N$   the number of examples) and model performance, enforcing that the line passes through the zero-shot performance point. This value approximates the amount of performance improvement from zero-shot expected from including an order of magnitude more demonstrating examples.  

Table 1:  Summary of benchmark datasets.  We use 10 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, molecular imagery) and tasks (multi-class, multi-label, and fine-grained classification).  

![](images/7205245d4d93aea120e20581767010eac80e677a2b8fa68813b14b64b9b19c77.jpg)  

![Table 2:  Many-shot ICL performance and efficiency comparison.  We report the performance under a zero-shot regime and performance at the optimal demo set size as well as the many-shot ICL data efficiency of GPT-4o and Gemini  $1.5~\mathrm{Pro}$  . We measure performance using accuracy on all datasets except CheXpert, for which we use macro-average F1. We bold the highest ICL data efficiency between the two models on each dataset. ](images/5747989fbf490aa5015d41f600c67f18638cc9a72f5f650cee24797078ec72b4.jpg)  

# 4 Results  

We present many-shot ICL performance using batched queries in Section 4.1, investigate the impact of batching queries on performance in Section 4.2, and provide an analysis on cost and latency in Section 4.3. Results using GPT4(V)-Turbo are in Appendix C.  

# 4.1 Increasing number of demonstrating examples  

Main Results.  Gemini 1.5 Pro exhibits consistent and substantial improvements as the number of demonstrating examples increases across all datasets except for DrugOOD Assay (Figure 2). Gemini  $1.5\;\mathrm{Pro}$   shows particularly large improvements from many-shot ICL on HAM10000 (  ${\it+23\%}$   accuracy compared to zero-shot,  $+16\%$   compared to 7 examples), FIVES   ${\it\Omega}+29\%$   compared to zero-shot,  $+27\%$  compared to 20 examples), and EuroSAT   $(+38\%$   compared to zero-shot,  $+31\%$   compared to 10 examples). Notably, for 5 out of the 10 datasets (FIVES, UCMerced, EuroSAT, Oxford Pets, and DTD), Gemini 1.5 Pro performance continues to improve up to the highest number of demonstrating examples considered   $(\mathord{\sim}1,\!000$   examples). On the other 5 datasets, the optimal performance occurs prior to the highest number of demo examples, with the maximum number of demo examples leading to similar or slightly worse performance than the optimal demo set size. On the other hand, Gemini 1.5 Pro performance on DrugOOD Assay does not substantially benefit from many-shot ICL, with high variance in performance across demo sizes and the peak performance at 40 demo examples.  

Similarly, GPT-4o shows substantial performance improvements on all datasets except FIVES and DrugOOD Assay using many-shot ICL, but the improvement is not consistent. For many datasets, performance drops sharply at first and then improves significantly as the number of demonstrating examples increases further, resulting in V-shaped scaling curves (Figure 2). We also note that we were unable to increase the number of demo examples to the same level as considered for Gemini 1.5 Pro because GPT-4o has a shorter context window and is more prone to timeout errors with longer inputs. GPT-4o performance on DrugOOD Assay shows high variance, similar to Gemini  $1.5\;\mathrm{Pro}$  , with the peak performance observed at 50 demo examples.  

Sensitivity to prompt selection.  We also explore a different set of prompts to test the robustness of many-shot ICL to differences in prompt wording on two datasets. While there is a small deviation in performance between different prompts, the overall log-linear improvement trend is consistent across the prompts. Details can be found in Appendix B.  

ICL data efficiency.  We find Gemini   $1.5\;\mathrm{Pro}$   demonstrates higher ICL data efficiency than GPT- 4o across all datasets except TerraIncognita and DTD (Table 2). Gemini 1.5 Pro ICL efficiency is especially high on EuroSAT, with  $20.61\%$   improvement in accuracy for every   $10\mathbf{x}$   more demo examples, and lowest on DrugOOD Assay (2.03), Camelyon17 (3.00), and TerraIncognita (3.50). GPT-4o ICL data efficiency is especially high on TerraIncognita   $(20.50\%)$   and EuroSat (19.40).  

![](images/bff391610e9c220cf73f2868432fb8d09d38554b2955e4c0cc37c21cee960108.jpg)  
Figure 3:  Gemini 1.5 Pro performance under many-shot and zero-shot ICL when varying the amount of queries included in every request.  We show performance per batch size with the optimal number of demo examples (many-shot) and no demo examples (zero-shot). The  $x$  -axis is in log scale. Under the many-shot regime, batching queries leads to no substantial drop in performance compared to individual queries when we choose a suitable batch size. For zero-shot, including only one query is suboptimal for many datasets.  

Gemini 1.5 Pro has a positive efficiency on all datasets and GPT-4o has a positive data efficiency on 9 of the 10 datasets (excluding Oxford Pets). Importantly, both models benefit substantially from many-shot ICL at the optimal demo set size, with an average improvement of  $+17\%$   for both Gemini 1.5 Pro and GPT-4o.  

# 4.2 Impact of batching queries  

As including a large set of demo examples in the prompt leads to much longer sequence lengths and therefore higher inference time and cost, we consider batching queries in a single prompt to reduce per-query cost, and examine the impact of different batch sizes on model performance. Due to its superior performance and free preview access, we use Gemini   $1.5\;\mathrm{Pro}$   for these experiments.  

Main Results.  We find minimal performance degradations, and sometimes performance improve- ments, as we increase the number of queries included in each batch across under both zero-shot and many-shot (at the optimal demo set size) regimes (Figure 3). Notably, using a single query each time with many-shot ICL is suboptimal across many of the datasets. We find that the optimal batch size is among the three largest sizes on every dataset except CheXpert and EuroSAT, which both see optimal performance with a single query at a time.  

We additionally observe that including a single query at a time is suboptimal on most datasets in the zero-shot regime. Surprisingly, performance with the highest batch size is substantially higher across three datasets under the zero-shot regime, with a consistent performance improvement as the batch size is increased on both UCMerced and Terraincognita.  

Zero-shot performance improvements from batching queries.  We conduct several additional experiments to investigate why batch querying can lead to large performance improvements under the zero-shot regime on TerraIncognita and UCMerced. We hypothesize that this improvement may be due to three potential benefits from ICL: (1) domain calibration, where the model benefits from seeing more images in the domain in order to adapt to it, (2) class calibration, where seeing images of different classes enables the model to better calibrate its outputs, and (3) self-ICL (shown to be effective in prior work [ 29 ]), where the model can learn from self-generated demonstrations due to autoregressive decoding. We design experiments to isolate the potential benefits from each of these types of ICL between asking a single query to batching 50 queries together.  

First, to measure potential improvement from domain calibration, we include 49 images from the same class in the prompt without including any label. We find a  $3.0\%$   improvement on TerraIncognita  

![](images/d448fc079ac55d9535c79b6dcb9cd01f084aad34cad1d391283da4d8103570e9.jpg)  
Figure 4:  Ablation study to investigate why batching queries leads to performance improve- ments when using Gemini  $\mathbf{1.5\,Pr0}$   in a zero-shot setting.  The first bar shows performance when including a single query, the second adds 49 unlabeled images from a single class, the third adds 49 unlabeled images in total from all classes, the fourth adds model responses to include self-generated demonstrations, and the last includes 50 queries in one request.  

and  $2.6\%$   degradation on UCMerced, suggesting domain calibration is helpful for the former but not the latter. Second, to capture performance gains from class calibration, we include a random sample of 49 images in the prompt, again without including the label. We see a further  $3.5\%$   improvement on TerraIncognita (  $6.5\%$   improvement from a single query) and a  $4.5\%$   improvement from a single query on UCMerced, suggesting including the context of class-balanced images is helpful even without labels. Third, to capture additional performance improvements from the self-generated labels, we obtain predicted labels from the zero-shot model using a single query for each of the 49 randomly sampled images and add them to the prompt. We observe further performance increase on both datasets, with  $5.5\%$   on TerraIncognita and  $2.7\%$   on UCMerced. The final total accuracy is similar to asking the 50 questions each round, which suggests these three components mostly explain the reason for improved zero-shot performance under a larger query batch size.  

# 4.3 Cost and latency analysis  

Many-shot ICL incurs zero additional training cost, but per-query inference can be costly and slow due to long input contexts. To quantitatively measure this, we compute the latency and cost associated with the zero-shot and many-shot requests with and without batching when using Gemini   $1.5\;\mathrm{Pro}$   on HAM10000 and TerraIncognita. We calculate the costs using the Gemini 1.5 Pro preview pricing (  $\S7$   per 1 million input tokens and   $\S21$   per 1 million output tokens). For fair comparison and to minimize data transfer artifacts, all requests are sent to the same location where the VM instance is held (“us-central1”). We run the query three times under each setting and report the average.  

In the zero-shot regime, we see substantial per-example latency reductions due to query batching, close to a  $10\mathbf{x}$   reduction on HAM10000 and  $2\mathbf{X}$   on TerraIncognita (Table 3). The per-example cost is similar between the two as there is no additional context needed for including demonstrating examples. In the many-shot regime, we observe substantial reductions in both per-example latency and cost on both datasets. Specifically, for HAM10000, we find a near  $35\mathrm{x}$   reduction in latency and  $10\mathbf{x}$   reduction in cost, and   $20\mathbf{x}$   reduction in latency and  $45\mathbf{x}$   reduction in cost for TerraIncognita.  

# 5 Discussion  

In this study, we evaluate many-shot ICL of state-of-the-art multimodal foundation models across 10 datasets and find consistent performance improvements across most of the datasets. Batching queries with many-shot ICL further exhibits substantially reduced per-example latency and inference costs without compromising performance.  

Our findings suggest that these multimodal foundation models have the capability of performing ICL with large numbers of demonstrating examples, which may have significant implications on their practical use. For example, it was previously impossible to adapt these large, private models to  

![Table 3:  Inference latency and cost using Gemini 1.5 Pro with and without query batching.  We use 50 queries per batch. In the zero-shot setting, we can achieve lower per-example latency with batching, but the per-example cost remains identical. In the many-shot setting, the per-example cost and per-example latency both drop substantially with query batching. ](images/08cad9838361c7c5255cbdae6c004feabab7f6d3d3cb985b2b076061aef4f1a1.jpg)  

new tasks and domains, but many-shot ICL would enable users to leverage demonstrating examples to adapt the models. One significant advantage of many-shot ICL is its ability to get quick results even on the same day of model release, and that’s why we can finish our evaluation using GPT-4o within days. Furthermore, fine-tuning open-source models is the standard practice when practitioners have access to moderately sized datasets, but many-shot ICL may remove the need for fine-tuning, making it much easier to develop customized approaches. We note that it remains to be seen how traditional fine-tuning of these models compares to many-shot ICL with foundation models in terms of absolute performance and data efficiency, so future work should explore this. In addition, it is important to study general issues which plague those foundation models, such as hallucinations and biases, under the context of many-shot ICL and batching queries. For example, it would be interesting to explore if carefully curated and large sets of demonstrating examples can reduce biases across different sub-groups. We leave this to future work.  

Our study has limitations. First, we only explore performance under many-shot ICL on image classification tasks and with private foundation models. We believe these are the most practically relevant and common multimodal settings, but it is worthwhile for future work to explore potential benefits from many-shot ICL on other tasks and with upcoming open-source multimodal foundation models like LLaMA-3 [ 30 ]. Second, even after recent developments to increase context size, the size prohibits many-shot ICL from being used on datasets with a large number (several hundred or more) of classes. We anticipate that context window sizes will continue to increase in size over time which will mitigate this issue. Third, the datasets which were used to train these private models have not been disclosed, so it is difficult to tell whether the models have been trained on the datasets we selected. We argue that zero-shot performance across the datasets is far from perfect which provides evidence that the datasets have not been used for training, but we cannot determine that with certainty.  

# 6 Conclusion  

In summary, we show that multimodal foundation models are capable of many-shot ICL. We believe that these results pave a promising path forward to improve the adaptability and accessibility of large multimodal foundation models.  

# References  

[1]  Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.  

[2]  Archit Parnami and Minwoo Lee. Learning from few examples: A summary of approaches to few-shot learning.  arXiv preprint arXiv:2203.04291 , 2022.  

[3]  Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples: A survey on few-shot learning.  ACM computing surveys (csur) , 53(3):1C34, 2020.

 [4]  Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.  arXiv preprint arXiv:2303.08774 , 2023.

 [5]  Zhongyi Han, Guanglin Zhou, Rundong He, Jindong Wang, Xing Xie, Tailin Wu, Yilong Yin, Salman Khan, Lina Yao, Tongliang Liu, et al. How well does gpt-4v (ision) adapt to distribution shifts? a preliminary investigation.  arXiv preprint arXiv:2312.07424 , 2023.

 [6]  Xingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia Hai, Renzhe Xu, Yuqing Yang, Shikai Guan, Jiazheng Xu, and Peng Cui. On the out-of-distribution generalization of multimodal large language models.  arXiv preprint arXiv:2402.06599 , 2024.

 [7]  Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, Zhiyong Wu, and Lingpeng Kong. In-context learning with many demonstration examples.  arXiv preprint arXiv:2302.04931 , 2023.

 [8]  Rishabh Agarwal, Avi Singh, Lei M Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John D Co-Reyes, Eric Chu, et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018 , 2024.

 [9]  Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R Gormley, and Graham Neubig. In-context learning with long-context models: An in-depth exploration.  arXiv preprint arXiv:2405.00200 , 2024.

 [10]  Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with multi-modal in-context learning.  arXiv preprint arXiv:2309.07915 , 2023.

 [11]  Zhoujun Cheng, Jungo Kasai, and Tao Yu. Batch prompting: Efficient inference with large language model apis.  arXiv preprint arXiv:2301.08721 , 2023.

 [12]  Jianzhe Lin, Maurice Diesendruck, Liang Du, and Robin Abraham. Batchprompt: Accomplish more with less.  arXiv preprint arXiv:2309.00384 , 2023.

 [13]  Jiayi Liu, Tinghan Yang, and Jennifer Neville. Cliqueparcel: An approach for batching llm prompts that jointly optimizes efficiency and faithfulness.  arXiv preprint arXiv:2402.14833 , 2024.

 [14]  Guijin Son, Sangwon Baek, Sangdae Nam, Ilgyun Jeong, and Seungone Kim. Multi-task inference: Can large language models follow multiple instructions at once?  arXiv preprint arXiv:2402.11597 , 2024.

 [15]  Siyu Xu, Yunke Wang, Daochang Liu, and Chang Xu. Collage prompting: Budget-friendly visual recognition with gpt-4v.  arXiv preprint arXiv:2403.11468 , 2024.

 [16]  Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean- baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.  arXiv preprint arXiv:2403.05530 , 2024.

 [17]  Yixuan Wu, Yizhou Wang, Shixiang Tang, Wenhao Wu, Tong He, Wanli Ouyang, Jian Wu, and Philip Torr. Dettoolchain: A new prompting paradigm to unleash detection ability of mllm. arXiv preprint arXiv:2403.12488 , 2024.

 [18]  Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection with multimodal large language models.  arXiv preprint arXiv:2305.18279 , 2023.

 [19]  Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions.  Scientific data , 5(1): 1C9, 2018.  

[20]  Kai Jin, Xingru Huang, Jingxing Zhou, Yunxiang Li, Yan Yan, Yibao Sun, Qianni Zhang, Yaqi Wang, and Juan Ye. Fives: A fundus image dataset for artificial intelligence based vessel segmentation.  Scientific Data , 9(1):475, 2022.

 [21]  Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In  Proceedings of the AAAI conference on artificial intelligence , volume 33, pages 590C597, 2019.

 [22]  Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge.  IEEE transactions on medical imaging , 38(2):550C560, 2018.

 [23]  Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In  Proceedings of the European conference on computer vision (ECCV) , pages 456C473, 2018.

 [24]  Yi Yang and Shawn Newsam. Bag-of-visual-words and spatial extensions for land-use clas- sification. In  Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems , pages 270C279, 2010.

 [25]  Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.  IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing , 12(7):2217C2226, 2019.

 [26]  Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In  2012 IEEE conference on computer vision and pattern recognition , pages 3498C3505. IEEE, 2012.

 [27]  Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 3606C3613, 2014.

 [28]  Yuanfeng Ji, Lu Zhang, Jiaxiang Wu, Bingzhe Wu, Long-Kai Huang, Tingyang Xu, Yu Rong, Lanqing Li, Jie Ren, Ding Xue, et al. Drugood: Out-of-distribution (ood) dataset curator and benchmark for ai-aided drug discoveryCa focus on affinity prediction problems with noise annotations.  arXiv preprint arXiv:2201.09637 , 2022.

 [29]  Wei-Lin Chen, Cheng-Kuang Wu, and Hsin-Hsi Chen. Self-icl: Zero-shot in-context learning with self-generated demonstrations.  arXiv preprint arXiv:2305.15035 , 2023.

 [30]  Introducing meta llama 3: The most capable openly available llm to date. URL  https: //ai.meta.com/blog/meta-llama-3/ .  

# A Prompts used for ICL experiments  

# A.1 Prompt used for image classification experiments  

prompt   $=$   "" for demo in demo_examples: prompt   $+=$   f"""  $<<\tt I M G>>G$  iven the image above, answer the following question- using the specified format. Question: What is in the image above? Choices: {str(class_desp)} Answer Choice: {demo.answer} """  

prompt   $+=$   f"""<<IMG  $>>$  Given the image above, answer the following question- using the specified format. Question: What is in the image above? Choices: {str(class_desp)}  

Please respond with the following format: ---BEGIN FORMAT TEMPLATE---Answer Choice: [Your Answer Choice Here] Confidence Score: [Your Numerical Prediction Confidence Score Here From 0 To 1] ---END FORMAT TEMPLATE---  

Do not deviate from the above format. Repeat the format template for the answer."""  

# A.2 Prompts used for image classification experiments with batching  

prompt   $=$   "" for demo in demo_examples: prompt   $+=$   f"""<<IMG>>Given the image above, answer the following question- using the specified format. Question: What is in the image above? Choices: {str(class_desp)} Answer Choice: {demo[1]}  

for idx, i in enumerate(test_df.iloc[start_idx:end_idx].itertuples()): prompt   $+=$   f"""<<IMG>>Given the image above, answer the following question- using the specified format. Question {qn_idx}: What is in the image above? Choices {qn_idx}: {str(class_desp)}  

"""  

for i in range(start_idx, end_idx): qn_idx   $=$   i-start_idx  $+1$  prompt   $+=$   f""" Please respond with the following format for each question: ---BEGIN FORMAT TEMPLATE FOR QUESTION {qn_idx}---Answer Choice {qn_idx}: [Your Answer Choice Here for Question {qn_idx}] Confidence Score {qn_idx}: [Your Numerical Prediction Confidence Score Here- From 0 To 1 for Question {qn_idx}] ---END FORMAT TEMPLATE FOR QUESTION {qn_idx}---  

Do not deviate from the above format. Repeat the format template for the answer."""  

# A.3 Prompts used for batching ablation experiments  

# A.3.1 Prefixing images  

prompt   $=$   "" for demo in prefix image paths: prompt   $+=$   f"""<<IMG>> """ prompt   $+=$   "Above are some images from the same dataset. " qns_idx  $\begin{array}{r l}{\mathbf{\Sigma}=}&{{}\left[\begin{array}{l l l}\end{array}\right]}\end{array}$  for idx, i in enumerate(test_df.iloc[start_idx:end_idx].itertuples()): qn_idx   $=$   idx  $^{+1}$  prompt   $+=$   f"""<<IMG>> Given the image above, answer the following question- using the specified format. Question {qn_idx}: What is in the image above? Choices {qn_idx}: {str(class_desp)} """ for i in range(start_idx, end_idx): qn_idx   $=$   i-start_idx  $+1$  prompt   $+=$   f""" Please respond with the following format for each question: ---BEGIN FORMAT TEMPLATE FOR QUESTION {qn_idx}---Answer Choice {qn_idx}: [Your Answer Choice Here for Question {qn_idx}] Confidence Score {qn_idx}: [Your Numerical Prediction Confidence Score Here- From 0 To 1 for Question {qn_idx}] ---END FORMAT TEMPLATE FOR QUESTION {qn_idx}---  

Do not deviate from the above format. Repeat the format template for the answer."""  

# B Prompt selection  

We utilize a different set of prompts to test the robustness of ManyICL to differences in prompt wording. We randomly sample two datasets (HAM10000 and EuroSAT) for this experiment due to budget limit.  

# B.1 Prompts used for prompt selection experiments  

Note that only the question section is shown here, and prompt 1 is used for all other image classifica- tion experiments.  

# B.1.1 Prompt 1  

<<IMG>>Given the image above, answer the following question using the specified format. Question {qn_idx}: What is in the image above? Choices {qn_idx}: {str(class_desp)}  

# B.1.2 Prompt 2  

<<IMG>>Given the image above, answer the following question using the specified format. Question {qn_idx}: Which class does this image belong to? Choices {qn_idx}: {str(class_desp)}  

# B.1.3 Prompt 3  

Question {qn_idx}: <<IMG>>Classify the image above, choose from {str(class_desp)}  

![](images/99b88c369400230a2a9184c28d29b1a0a33e58585e37cb97682df3efb971c013.jpg)  
Figure 5:  Sensitivity analysis of many-shot ICL.  These plots show the change in task performance on two datasets as the number of demonstrating examples increases, using three different prompts. For all experiments on sensitivity analysis, the Gemini 1.5 Pro model is used. The  $x$  -axis is in the logarithmic scale, representing the number of demonstrating examples plus one. The log-linear improvement until the optimal performance is consistent across all prompts selected.  

# B.2 Prompt selection results  

Figure 5 shows the sensitivity of performance to prompt selection on two datasets with three prompts. While there exists a small deviation in performance, but the overall log-linear improvement trend is consistent.  

# C GPT4(V)-Turbo performance under many-shot ICL  

GPT4(V)-Turbo shows mixed results for many-shot ICL, with substantial performance improvements on HAM1000, UCMerced, EuroSAT, and DTD, but minimal improvements or no improvement across the other six datasets (Figure 6). However, we note that we were unable to increase the number of demo examples to the same level as Gemini 1.5 Pro because GPT4(V)-Turbo has a shorter context window and is more prone to timeout errors when scaling. Additionally, GPT4(V)-Turbo seems to generally underperform Gemini   $1.5\;\mathrm{Pro}$   across the datasets excluding FIVES and EuroSAT for which it seems to mostly match the Gemini 1.5 Pro performance. GPT4(V)-Turbo performance on DrugOOD Assay shows high variance, resembling that of Gemini 1.5 Pro with the peak performance at 40 demo examples.  

# D Performance of many-shot ICL on medical QA tasks  

# D.1 Prompt used for medical QA experiments (MedQA, MedMCQA)  

prompt   $=$   "You are an expert in answering medical exam questions. " for demo in demo_examples: prompt   $+=$   f"""Question: {demo.question} Choices: {demo.options} Answer: {demo.answer}  

prompt   $+=$   f"""Question: {actual.question} Choices: {actual.options}  

![](images/7fbb1117324c771ddde9abff02886943aa7c4fe74670ff3b2eecb97784e88c53.jpg)  
Figure 6:  GPT4(V)-Turbo and GPT-4o performance from zero-shot to many-shot ICL.  X-axis is in log scale.  

![](images/7fa5e3c838b281a806b1b13a640b9052e8547e81867ca302096902f370c707f6.jpg)  
Figure 7: Many-shot ICL performances of medical QA tasks.  

Do not deviate from the above format. Repeat the format template for the answer."""  

# D.2 Results  

Figure 7 shows the results on medical QA tasks.  

# Acknowledgments and Disclosure of Funding  

We thank Dr. Jeff Dean, Yuhui Zhang, Dr. Mutallip Anwar, Kefan Dong, Rishi Bommasani, Ravi B. Sojitra, Chen Shani and Annie Chen for their feedback on the ideas and manuscript. Yixing Jiang is supported by National Science Scholarship (PhD). This work is also supported by Google cloud credit. Dr. Jonathan Chen has received research funding support in part by NIH/National Institute of Allergy and Infectious Diseases (1R01AI17812101), NIH/National Institute on Drug Abuse Clinical Trials Network (UG1DA015815 - CTN-0136), Gordon and Betty Moore Foundation (Grant #12409), Stanford Artificial Intelligence in Medicine and Imaging - Human-Centered Artificial Intelligence (AIMI-HAI) Partnership Grant, Google, Inc. Research collaboration Co-I to leverage EHR data to predict a range of clinical outcomes, American Heart Association - Strategically Focused Research Network - Diversity in Clinical Trials and NIH-NCATS-CTSA grant (UL1TR003142) for common research resources.  
2024-08-19 17:03:20,714 - ERROR - Task exception was never retrieved
future: <Task finished name='Task-1' coro=<translate_markdown() done, defined at D:\GitHub\MinerU\translate\markdown_translator.py:210> exception=KeyboardInterrupt()>
Traceback (most recent call last):
  File "D:\Program Files\JetBrains\PyCharm Community Edition 2024.1\plugins\python-ce\helpers\pydev\pydevd.py", line 2236, in <module>
    main()
  File "D:\Program Files\JetBrains\PyCharm Community Edition 2024.1\plugins\python-ce\helpers\pydev\pydevd.py", line 2218, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File "D:\Program Files\JetBrains\PyCharm Community Edition 2024.1\plugins\python-ce\helpers\pydev\pydevd.py", line 1528, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File "D:\Program Files\JetBrains\PyCharm Community Edition 2024.1\plugins\python-ce\helpers\pydev\pydevd.py", line 1535, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "D:\Program Files\JetBrains\PyCharm Community Edition 2024.1\plugins\python-ce\helpers\pydev\_pydev_imps\_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "D:\GitHub\MinerU\translate\main.py", line 79, in <module>
    process_markdown_file(path, "英语", "汉语", "中国")
  File "D:\GitHub\MinerU\translate\main.py", line 56, in process_markdown_file
    translation = asyncio.run(translate_markdown(
  File "D:\Program Files\anaconda3\envs\MinerU\lib\asyncio\runners.py", line 47, in run
    _cancel_all_tasks(loop)
  File "D:\Program Files\anaconda3\envs\MinerU\lib\asyncio\runners.py", line 63, in _cancel_all_tasks
    loop.run_until_complete(tasks.gather(*to_cancel, return_exceptions=True))
  File "D:\Program Files\anaconda3\envs\MinerU\lib\asyncio\base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "D:\Program Files\anaconda3\envs\MinerU\lib\asyncio\windows_events.py", line 321, in run_forever
    super().run_forever()
  File "D:\Program Files\anaconda3\envs\MinerU\lib\asyncio\base_events.py", line 603, in run_forever
    self._run_once()
  File "D:\Program Files\anaconda3\envs\MinerU\lib\asyncio\base_events.py", line 1909, in _run_once
    handle._run()
  File "D:\Program Files\anaconda3\envs\MinerU\lib\asyncio\events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
  File "D:\GitHub\MinerU\translate\markdown_translator.py", line 250, in translate_markdown
  File "D:\Program Files\anaconda3\envs\MinerU\lib\asyncio\runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "D:\Program Files\anaconda3\envs\MinerU\lib\asyncio\base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "D:\Program Files\anaconda3\envs\MinerU\lib\asyncio\windows_events.py", line 321, in run_forever
    super().run_forever()
  File "D:\Program Files\anaconda3\envs\MinerU\lib\asyncio\base_events.py", line 603, in run_forever
    self._run_once()
  File "D:\Program Files\anaconda3\envs\MinerU\lib\asyncio\base_events.py", line 1909, in _run_once
    handle._run()
  File "D:\Program Files\anaconda3\envs\MinerU\lib\asyncio\events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
  File "D:\GitHub\MinerU\translate\markdown_translator.py", line 206, in run_with_semaphore
  File "D:\GitHub\MinerU\translate\markdown_translator.py", line 81, in one_chunk_translation
  File "D:\GitHub\MinerU\translate\markdown_translator.py", line 30, in get_completion
  File "D:\Program Files\anaconda3\envs\MinerU\lib\asyncio\base_events.py", line 808, in run_in_executor
    self._check_closed()
  File "D:\Program Files\anaconda3\envs\MinerU\lib\asyncio\base_events.py", line 808, in run_in_executor
    self._check_closed()
  File "_pydevd_bundle\\pydevd_cython_win32_310_64.pyx", line 1187, in _pydevd_bundle.pydevd_cython_win32_310_64.SafeCallWrapper.__call__
  File "_pydevd_bundle\\pydevd_cython_win32_310_64.pyx", line 627, in _pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch
  File "_pydevd_bundle\\pydevd_cython_win32_310_64.pyx", line 1103, in _pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch
  File "_pydevd_bundle\\pydevd_cython_win32_310_64.pyx", line 1065, in _pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch
  File "_pydevd_bundle\\pydevd_cython_win32_310_64.pyx", line 585, in _pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.do_wait_suspend
  File "D:\Program Files\JetBrains\PyCharm Community Edition 2024.1\plugins\python-ce\helpers\pydev\pydevd.py", line 1185, in do_wait_suspend
    self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread)
  File "D:\Program Files\JetBrains\PyCharm Community Edition 2024.1\plugins\python-ce\helpers\pydev\pydevd.py", line 1200, in _do_wait_suspend
    time.sleep(0.01)
KeyboardInterrupt
2024-08-20 11:28:48,259 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: [Errno 2] No such file or directory: 'D:\\GitHub\\MinerU\\translate\\images/c2295c09026975f63765663f5663420aa41ecabe03fed3062728054bfde7fa4a.jpg'
2024-08-20 11:38:02,532 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: [WinError 3] 系统找不到指定的路径。: 'C:\\Users\\yongjie.yang\\Desktop\\2405.09798v1\\2405.09798v1 - 副本.md\\image_cache'
2024-08-20 11:38:13,794 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: [WinError 3] 系统找不到指定的路径。: 'C:\\Users\\yongjie.yang\\Desktop\\2405.09798v1\\2405.09798v1 - 副本.md\\image_cache'
2024-08-20 15:01:55,657 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 15:07:00,597 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 15:08:14,065 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 15:11:53,360 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 15:13:10,241 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 15:14:16,716 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 15:16:01,514 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 15:17:43,986 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 15:18:23,293 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 15:19:54,976 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 15:22:11,432 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 15:26:38,516 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 15:29:13,933 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 15:31:04,099 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 15:34:03,994 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 15:34:25,351 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 15:35:17,649 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 15:45:43,192 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\2405.09798v1\2405.09798v1 - 副本.md: bad escape \c at position 7
2024-08-20 16:40:44,114 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\测试\1.md: 'latin-1' codec can't encode characters in position 57-63: ordinal not in range(256)
2024-08-20 17:44:11,606 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\增强 ChatGPT 提示工程的提示模式目录\增强 ChatGPT 提示工程的提示模式目录.md: TTF Font file not found: path_to_your_Arial_Unicode_MS.ttf
2024-08-21 10:58:18,383 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\一张图片价值170个Token：GPT-4是如何编码图像的？\一张图片价值170个Token：GPT-4是如何编码图像的？.md: TTF Font file not found: path_to_your_Arial_Unicode_MS.ttf
2024-08-21 12:01:17,720 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\一张图片价值170个Token：GPT-4是如何编码图像的？\一张图片价值170个Token：GPT-4是如何编码图像的？.md: TTF Font file not found: path_to_your_Arial_Unicode_MS.ttf
2024-08-21 14:19:38,590 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\1\A Picture is Worth 170 Tokens How Does GPT-4o Encode Images.md: TTF Font file not found: path_to_your_Arial_Unicode_MS.ttf
2024-08-21 14:27:21,155 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\1\A Picture is Worth 170 Tokens How Does GPT-4o Encode Images.md: TTF Font file not found: path_to_your_Arial_Unicode_MS.ttf
2024-08-21 16:45:30,596 - ERROR - Error ID: fb3f6768-eb9c-410b-9220-f7061086190a - Error: Unexpected response format.
2024-08-21 16:45:58,494 - ERROR - Error ID: 9a193219-bdfc-4282-8ee4-09bd2569c344 - Error: client_qwen returned None
2024-08-21 16:48:34,471 - ERROR - Error ID: 48414121-58d5-4c69-addf-878491a4e970 - Error: Unexpected response format.
2024-08-21 16:48:35,768 - ERROR - Error ID: b567325c-4969-4f92-a318-1a15da308db8 - Error: client_qwen returned None
2024-08-21 16:49:25,292 - ERROR - Error ID: 2417a239-5576-4f0d-9904-5f6f8d3fbd28 - Error: Unexpected response format.
2024-08-21 16:52:20,446 - ERROR - Error ID: b07ea973-f36d-40a9-b3d4-237460c725ce - Error: Unexpected response format.
2024-08-21 16:53:22,874 - ERROR - Error ID: 5777bd92-066e-412a-a944-d19e82cbfa13 - Error: Unexpected response format.
2024-08-21 16:54:24,213 - ERROR - Error ID: 17af08d6-d47b-4caa-a06c-be64c912a5b0 - Error: Unexpected response format.
2024-08-21 16:54:53,789 - ERROR - Error ID: 9146663e-d744-40fc-aca2-bb6418959d83 - Error: Unexpected response format.
2024-08-21 16:55:21,946 - ERROR - Error ID: e86ab45b-7914-4b3e-bd33-8378c2b098a1 - Error: Unexpected response format.
2024-08-21 16:55:49,039 - ERROR - Error ID: 9c41d910-d59f-4f73-8a56-f71d58abcf8c - Error: Unexpected response format.
2024-08-21 16:56:18,759 - ERROR - Error ID: 1b439feb-f6be-4666-a8ef-73ced760e75a - Error: Unexpected response format.
2024-08-21 16:56:24,042 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\1\A Picture is Worth 170 Tokens How Does GPT-4o Encode Images.md: TTF Font file not found: path_to_your_Arial_Unicode_MS.ttf
2024-08-22 15:54:19,061 - ERROR - Error ID: 7beb6fd8-669e-482d-a48f-58f3199f5a34 - Error: client_qwen returned None
2024-08-22 15:54:19,067 - ERROR - Error ID: b94d097a-8d24-4717-a427-c09b0d068dd2 - Error: client_qwen returned None
2024-08-22 15:54:19,067 - ERROR - Error ID: 6d68ee15-8c31-4651-8c93-f94e9358e896 - Error: client_qwen returned None
2024-08-22 15:55:01,651 - ERROR - Error ID: 7cceedcb-1601-45d5-b85f-dead106f33bd - Error: Unexpected response format.
2024-08-22 15:57:24,220 - ERROR - Error ID: 3e8f0b94-766c-4065-863b-0d6db38a6a18 - Error: client_qwen returned None
2024-08-22 15:57:24,242 - ERROR - Error ID: 11617580-9cb8-46ef-8885-60fdfb37c4cd - Error: client_qwen returned None
2024-08-22 15:57:36,698 - ERROR - Error ID: 18bb81c9-20e0-4695-b206-786efa69978d - Error: Unexpected response format.
2024-08-22 16:00:29,394 - ERROR - Error ID: 6051232b-18c5-4692-9983-54420af3acd7 - Error: client_qwen returned None
2024-08-22 16:00:29,413 - ERROR - Error ID: e1d5a164-8925-4715-aab9-8ea7c52aa505 - Error: client_qwen returned None
2024-08-22 16:16:30,309 - ERROR - Error ID: 2886cf99-8b36-4f25-b1fc-3b31b17fbdd1 - Error: client_qwen returned None
2024-08-22 16:16:30,317 - ERROR - Error ID: 57c4947e-1918-4be5-9b18-eabffa6ab9ef - Error: client_qwen returned None
2024-08-22 16:16:56,022 - ERROR - Error ID: 38de4fe1-0bfc-4f43-b56f-e738298962e8 - Error: client_qwen returned None
2024-08-22 16:17:03,818 - ERROR - Error ID: 59bd90b3-5cc8-432b-93e9-34e8c32096ff - Error: client_qwen returned None
2024-08-22 16:17:09,243 - ERROR - Error ID: bee90c71-7203-413a-9a48-6d2fe85a38b1 - Error: client_qwen returned None
2024-08-22 16:23:39,477 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\What Is DSPy How It Works\What Is DSPy How It Works.md: TTF Font file not found: path_to_your_Arial_Unicode_MS.ttf
2024-08-22 17:06:48,167 - ERROR - Error processing file C:\Users\yongjie.yang\Desktop\What Is DSPy How It Works\What Is DSPy How It Works.md: TTF Font file not found: path_to_your_Arial_Unicode_MS.ttf
